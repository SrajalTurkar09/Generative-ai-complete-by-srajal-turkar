{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# 1. Dataset preparation (CIFAR-10 as an example)\n",
        "def load_data():\n",
        "    (train_images, _), (_, _) = tf.keras.datasets.cifar10.load_data()  # Load the CIFAR-10 dataset\n",
        "    train_images = train_images.astype(\"float32\") / 255.0  # Normalize images to [0, 1]\n",
        "    return train_images\n",
        "\n",
        "train_images = load_data()\n",
        "\n",
        "# 2. Vision Transformer (ViT) model\n",
        "class ViTImageGenerator(tf.keras.Model):\n",
        "    def __init__(self, img_size=32, patch_size=4, embed_dim=64, num_heads=4, num_layers=6):\n",
        "        super(ViTImageGenerator, self).__init__()\n",
        "\n",
        "        # Parameters for the patch embedding\n",
        "        self.patch_size = patch_size\n",
        "        num_patches = (img_size // patch_size) ** 2  # Total number of patches\n",
        "\n",
        "        # Dense layer to embed the patches\n",
        "        self.patch_embed = layers.Dense(embed_dim)\n",
        "\n",
        "        # Positional embedding to retain spatial information\n",
        "        self.position_embed = tf.Variable(tf.random.normal([num_patches, embed_dim]))\n",
        "\n",
        "        # Transformer layers for processing the patches\n",
        "        self.encoder_layers = [\n",
        "            layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim) for _ in range(num_layers)\n",
        "        ]\n",
        "        self.norm_layers = [layers.LayerNormalization() for _ in range(num_layers)]\n",
        "\n",
        "        # Output layer to generate the image (scaled to [0,1] using sigmoid)\n",
        "        self.fc = layers.Dense(img_size * img_size * 3, activation='sigmoid')\n",
        "\n",
        "    def call(self, x):\n",
        "        # Step 1: Extract patches from the input image\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=x,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],  # Patch size\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],  # Move by patch size\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding='VALID'\n",
        "        )\n",
        "\n",
        "        # Get the batch size dynamically using tf.shape\n",
        "        batch_size = tf.shape(x)[0]\n",
        "\n",
        "        # Reshape patches to [batch, patches, channels], using batch_size\n",
        "        patches = tf.reshape(patches, (batch_size, -1, patches.shape[-1]))\n",
        "\n",
        "        # Step 2: Embed patches and add positional information\n",
        "        x = self.patch_embed(patches)\n",
        "        x += self.position_embed  # Adding position embedding\n",
        "\n",
        "        # Step 3: Pass the embedded patches through transformer layers\n",
        "        for enc_layer, norm_layer in zip(self.encoder_layers, self.norm_layers):\n",
        "            x = norm_layer(x + enc_layer(x, x))  # Apply attention + residual connection\n",
        "\n",
        "        # Step 4: Reconstruct the image from the processed patches\n",
        "        x = self.fc(x)\n",
        "        return tf.reshape(x, (-1, 32, 32, 3))  # Reshape the output back to image dimensions\n",
        "\n",
        "# 3. Model training\n",
        "def train_vit():\n",
        "    # Instantiate the ViT model\n",
        "    vit_model = ViTImageGenerator()\n",
        "    vit_model.compile(optimizer='adam', loss='mse')  # Compile the model with Adam optimizer and MSE loss\n",
        "\n",
        "    # Train the model (autoencoder style) to reconstruct the input image\n",
        "    vit_model.fit(train_images, train_images, epochs=10, batch_size=64)\n",
        "\n",
        "train_vit()  # Start training\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AsfyCzqcTM75",
        "outputId": "13058080-d30e-4e85-9ba4-bb28d80d2347"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node compile_loss/mse/sub defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-8-245532aafb4d>\", line 73, in <cell line: 73>\n\n  File \"<ipython-input-8-245532aafb4d>\", line 71, in train_vit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 318, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 54, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py\", line 357, in _compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py\", line 325, in compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/compile_utils.py\", line 609, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/compile_utils.py\", line 645, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/loss.py\", line 43, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py\", line 27, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py\", line 1286, in mean_squared_error\n\nIncompatible shapes: [64,32,32,3] vs. [4096,32,32,3]\n\t [[{{node compile_loss/mse/sub}}]] [Op:__inference_one_step_on_iterator_63275]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-245532aafb4d>\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mvit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mtrain_vit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-245532aafb4d>\u001b[0m in \u001b[0;36mtrain_vit\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# Train the model (autoencoder style) to reconstruct the input image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mvit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mtrain_vit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node compile_loss/mse/sub defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-8-245532aafb4d>\", line 73, in <cell line: 73>\n\n  File \"<ipython-input-8-245532aafb4d>\", line 71, in train_vit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 318, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 54, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py\", line 357, in _compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/trainer.py\", line 325, in compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/compile_utils.py\", line 609, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/compile_utils.py\", line 645, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/loss.py\", line 43, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py\", line 27, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py\", line 1286, in mean_squared_error\n\nIncompatible shapes: [64,32,32,3] vs. [4096,32,32,3]\n\t [[{{node compile_loss/mse/sub}}]] [Op:__inference_one_step_on_iterator_63275]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# 1. Dataset preparation (CIFAR-10 as an example)\n",
        "def load_data():\n",
        "    (train_images, _), (_, _) = tf.keras.datasets.cifar10.load_data()  # Load the CIFAR-10 dataset\n",
        "    train_images = train_images.astype(\"float32\") / 255.0  # Normalize images to [0, 1]\n",
        "    return train_images\n",
        "\n",
        "train_images = load_data()\n",
        "\n",
        "# 2. Vision Transformer (ViT) model\n",
        "class ViTImageGenerator(tf.keras.Model):\n",
        "    def __init__(self, img_size=32, patch_size=4, embed_dim=64, num_heads=4, num_layers=6):\n",
        "        super(ViTImageGenerator, self).__init__()\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        num_patches = (img_size // patch_size) ** 2  # Total number of patches\n",
        "\n",
        "        # Dense layer to embed the patches\n",
        "        self.patch_embed = layers.Dense(embed_dim)\n",
        "\n",
        "        # Positional embedding to retain spatial information\n",
        "        self.position_embed = tf.Variable(tf.random.normal([num_patches, embed_dim]))\n",
        "\n",
        "        # Transformer layers for processing the patches\n",
        "        self.encoder_layers = [\n",
        "            layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim) for _ in range(num_layers)\n",
        "        ]\n",
        "        self.norm_layers = [layers.LayerNormalization() for _ in range(num_layers)]\n",
        "\n",
        "        # Output layer to generate the image (scaled to [0,1] using sigmoid)\n",
        "        self.fc = layers.Dense(patch_size * patch_size * 3, activation='sigmoid') # Output per patch\n",
        "\n",
        "    def call(self, x):\n",
        "        # Step 1: Extract patches from the input image\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=x,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],  # Patch size\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],  # Move by patch size\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding='VALID'\n",
        "        )\n",
        "\n",
        "        # Get the batch size dynamically using tf.shape\n",
        "        batch_size = tf.shape(x)[0]\n",
        "\n",
        "        # Reshape patches to [batch, patches, channels], using batch_size\n",
        "        patches = tf.reshape(patches, (batch_size, -1, patches.shape[-1]))\n",
        "\n",
        "        # Step 2: Embed patches and add positional information\n",
        "        x = self.patch_embed(patches)\n",
        "        x += self.position_embed  # Adding position embedding\n",
        "\n",
        "        # Step 3: Pass the embedded patches through transformer layers\n",
        "        for enc_layer, norm_layer in zip(self.encoder_layers, self.norm_layers):\n",
        "            x = norm_layer(x + enc_layer(x, x))  # Apply attention + residual connection\n",
        "\n",
        "        # Step 4: Reconstruct the image from the processed patches\n",
        "        x = self.fc(x) # Apply fc layer to each patch\n",
        "\n",
        "        # Reshape the output to image dimensions\n",
        "        x = tf.reshape(x, (batch_size, self.img_size // self.patch_size, self.img_size // self.patch_size, self.patch_size * self.patch_size * 3))\n",
        "\n",
        "        # Reshape to the original image size\n",
        "        reconstructed = tf.nn.depth_to_space(x, self.patch_size)\n",
        "\n",
        "        return reconstructed # Reshape the output back to image dimensions\n",
        "\n",
        "# 3. Model training\n",
        "def train_vit():\n",
        "    # Instantiate the ViT model\n",
        "    vit_model = ViTImageGenerator()\n",
        "    vit_model.compile(optimizer='adam', loss='mse')  # Compile the model with Adam optimizer and MSE loss\n",
        "\n",
        "    # Train the model (autoencoder style) to reconstruct the input image\n",
        "    vit_model.fit(train_images, train_images, epochs=10, batch_size=32)\n",
        "\n",
        "    return vit_model\n",
        "\n",
        "# Train the model\n",
        "vit_model = train_vit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIEaQcgSUcuc",
        "outputId": "467088df-feed-4fb6-a880-45db97f48b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m 129/1563\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:49\u001b[0m 327ms/step - loss: 0.0682"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DdERuuMxYyBz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}