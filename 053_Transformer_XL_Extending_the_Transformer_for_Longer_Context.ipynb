{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import TensorFlow and NumPy libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "-yxId6-HJ-QX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_positional_encoding(seq_len, d_model):\n",
        "    \"\"\"\n",
        "    Generates positional encoding using sinusoidal patterns to indicate token position.\n",
        "\n",
        "    Args:\n",
        "        seq_len (int): Length of the sequence\n",
        "        d_model (int): Embedding dimension size\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: A tensor of shape (seq_len, d_model) with positional encodings\n",
        "    \"\"\"\n",
        "    # Compute position and divide terms for the encoding\n",
        "    position = np.arange(seq_len)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "\n",
        "    # Apply sine and cosine functions to even and odd indices respectively\n",
        "    pos_encoding = np.zeros((seq_len, d_model))\n",
        "    pos_encoding[:, 0::2] = np.sin(position * div_term)\n",
        "    pos_encoding[:, 1::2] = np.cos(position * div_term)\n",
        "\n",
        "    return tf.constant(pos_encoding, dtype=tf.float32)\n"
      ],
      "metadata": {
        "id": "IiMlncpNJ-To"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerXL:\n",
        "    \"\"\"\n",
        "    A basic Transformer-XL model for long-context sequence modeling.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads, num_layers, memory_len, max_len):\n",
        "        \"\"\"\n",
        "        Initializes model hyperparameters and weights.\n",
        "\n",
        "        Args:\n",
        "            d_model (int): Dimension of the model embeddings\n",
        "            num_heads (int): Number of attention heads\n",
        "            num_layers (int): Number of Transformer layers\n",
        "            memory_len (int): Length of memory to retain past context\n",
        "            max_len (int): Maximum length of input sequences\n",
        "        \"\"\"\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.memory_len = memory_len\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Positional encoding for each token position\n",
        "        self.positional_encoding = get_positional_encoding(max_len, d_model)\n",
        "\n",
        "        # Initialize weight matrices for query, key, value, and output transformations\n",
        "        self.wq = tf.Variable(tf.random.normal([d_model, d_model]), trainable=True)\n",
        "        self.wk = tf.Variable(tf.random.normal([d_model, d_model]), trainable=True)\n",
        "        self.wv = tf.Variable(tf.random.normal([d_model, d_model]), trainable=True)\n",
        "        self.wo = tf.Variable(tf.random.normal([d_model, d_model]), trainable=True)\n",
        "\n",
        "        # Feed-forward network weights for processing outputs\n",
        "        self.ffn_w1 = tf.Variable(tf.random.normal([d_model, d_model * 4]), trainable=True)\n",
        "        self.ffn_w2 = tf.Variable(tf.random.normal([d_model * 4, d_model]), trainable=True)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        \"\"\"\n",
        "        Splits the embedding tensor into multiple attention heads.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): Input tensor of shape (batch_size, seq_len, d_model)\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Reshaped tensor with multiple heads for parallel attention\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, depth = x.shape\n",
        "        x = tf.reshape(x, (batch_size, seq_len, self.num_heads, depth // self.num_heads))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])  # [batch_size, num_heads, seq_len, depth_per_head]\n",
        "\n",
        "    def attention(self, q, k, v):\n",
        "        \"\"\"\n",
        "        Calculates scaled dot-product attention scores and applies them.\n",
        "\n",
        "        Args:\n",
        "            q, k, v (tf.Tensor): Query, key, and value tensors for attention\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Output after applying attention scores\n",
        "        \"\"\"\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_logits = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(dk)\n",
        "        attention_weights = tf.nn.softmax(scaled_logits, axis=-1)\n",
        "        return tf.matmul(attention_weights, v)\n",
        "\n",
        "    def forward(self, x, memory):\n",
        "        \"\"\"\n",
        "        Forward pass of the Transformer-XL model, with memory handling.\n",
        "\n",
        "        Args:\n",
        "            x (tf.Tensor): Input tensor of shape (batch_size, seq_len, d_model)\n",
        "            memory (tf.Tensor): Memory tensor of past context to extend sequence length\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Model output after self-attention and memory update\n",
        "        \"\"\"\n",
        "        # Add positional encoding to input\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = x + self.positional_encoding[:seq_len, :]\n",
        "\n",
        "        for layer in range(self.num_layers):\n",
        "            # Combine memory from previous segments with current sequence\n",
        "            x_combined = tf.concat([memory, x], axis=1)\n",
        "\n",
        "            # Compute queries, keys, and values\n",
        "            q = tf.matmul(x, self.wq)\n",
        "            k = tf.matmul(x_combined, self.wk)\n",
        "            v = tf.matmul(x_combined, self.wv)\n",
        "\n",
        "            # Split for multi-head attention\n",
        "            q, k, v = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n",
        "\n",
        "            # Apply attention mechanism\n",
        "            attention_output = self.attention(q, k, v)\n",
        "            attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
        "            concat_attention = tf.reshape(attention_output, (x.shape[0], -1, self.d_model))\n",
        "            attention_output = tf.matmul(concat_attention, self.wo)\n",
        "\n",
        "            # Add residual connection and layer normalization\n",
        "            layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "            x = layernorm1(x + attention_output)\n",
        "\n",
        "            # Feed-forward layer with activation and layer normalization\n",
        "            ffn_output = tf.nn.relu(tf.matmul(x, self.ffn_w1))\n",
        "            ffn_output = tf.matmul(ffn_output, self.ffn_w2)\n",
        "            layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "            x = layernorm2(x + ffn_output)\n",
        "\n",
        "            # Update memory with current output, trimming to memory_len\n",
        "            memory = tf.concat([memory, x], axis=1)[:, -self.memory_len:]\n",
        "\n",
        "        return x, memory  # Return output and updated memory\n"
      ],
      "metadata": {
        "id": "uv6wkwJBJ-aP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "d_model = 512          # Dimension of token embeddings\n",
        "num_heads = 8          # Number of attention heads\n",
        "num_layers = 4         # Number of transformer layers\n",
        "memory_len = 128       # Length of memory for context\n",
        "max_len = 512          # Maximum sequence length\n",
        "\n",
        "# Create an instance of the Transformer-XL model\n",
        "model = TransformerXL(d_model, num_heads, num_layers, memory_len, max_len)\n",
        "\n",
        "# Example input data and initial memory\n",
        "input_data = tf.random.normal([1, 100, d_model])  # (batch_size, seq_len, d_model)\n",
        "memory = tf.zeros([1, 0, d_model])  # Initial empty memory\n",
        "\n",
        "# Perform a forward pass\n",
        "output, updated_memory = model.forward(input_data, memory)\n"
      ],
      "metadata": {
        "id": "VPFX8cyBJ-dG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the output and updated memory shapes\n",
        "print(\"Model Output Shape:\", output.shape)          # Expected shape: (batch_size, seq_len, d_model)\n",
        "print(\"Updated Memory Shape:\", updated_memory.shape)  # Expected shape: (batch_size, memory_len, d_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AagG7oNYJ-ge",
        "outputId": "cbbfc840-a0a2-473a-a766-121c8f5bc1bd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output Shape: (1, 100, 512)\n",
            "Updated Memory Shape: (1, 128, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WZxUWWb7J-jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B7X_-I6YJ-m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mVkJaXpxJ-qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iSrcSLOxJ-t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WiPEwC7gJ-xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r9cqF1xwJ-0P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}