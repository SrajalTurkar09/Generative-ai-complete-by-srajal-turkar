{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IteS3TMRqG8Z"
      },
      "source": [
        "# SAGAN (Self-Attention GAN) Implementation\n",
        "This notebook implements a Self-Attention GAN (SAGAN) for image generation using the CIFAR-10 dataset. Every section is thoroughly commented for clarity and serves as a reference for future GAN-based projects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "91lNOGZ-qG8c"
      },
      "outputs": [],
      "source": [
        "# Import PyTorch and its submodules for building models and data loading\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, utils  # For dataset and image utilities\n",
        "import matplotlib.pyplot as plt  # For visualizing generated images\n",
        "import os  # For file and directory handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA0gxdOpqG8e",
        "outputId": "b7ee83d5-6e6b-4664-bdad-4d7ef377072f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 35.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Dataset loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Define image preprocessing steps: resize, normalize, and convert to tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(32),  # Resize images to 32x32\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to the range [-1, 1]\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 training dataset with the defined transformations\n",
        "dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
        "\n",
        "# Use a DataLoader to fetch the data in batches for training\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "print(\"Dataset loaded successfully!\")  # Confirmation that the dataset is ready\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RAqPm2_PqG8e"
      },
      "outputs": [],
      "source": [
        "# Step 3: Define the Self-Attention Layer\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        # Reduce dimensionality for attention computation\n",
        "        self.query = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.key = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)\n",
        "        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "\n",
        "        # Scaling factor to control attention contribution\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch, channels, height, width = x.size()  # Get the input dimensions\n",
        "\n",
        "        # Compute query, key, and value feature maps\n",
        "        query = self.query(x).view(batch, -1, height * width).permute(0, 2, 1)  # (B, HW, C//8)\n",
        "        key = self.key(x).view(batch, -1, height * width)  # (B, C//8, HW)\n",
        "        attention = torch.softmax(torch.bmm(query, key), dim=-1)  # Attention map: (B, HW, HW)\n",
        "\n",
        "        value = self.value(x).view(batch, -1, height * width)  # (B, C, HW)\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1))  # Weighted value: (B, C, HW)\n",
        "        out = out.view(batch, channels, height, width)  # Reshape to match input\n",
        "\n",
        "        return self.gamma * out + x  # Combine input and attention-enhanced output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7HxYN26vqG8f"
      },
      "outputs": [],
      "source": [
        "# Step 4: Define the Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, img_channels, feature_maps):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Define the starting size of the image (before upsampling)\n",
        "        self.init_size = 4\n",
        "\n",
        "        # Fully connected layer to map noise to initial features\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(z_dim, feature_maps * 8 * self.init_size * self.init_size)\n",
        "        )\n",
        "\n",
        "        # Define the generator network\n",
        "        self.gen = nn.Sequential(\n",
        "            nn.BatchNorm2d(feature_maps * 8),  # Normalize initial features\n",
        "\n",
        "            nn.Upsample(scale_factor=2),  # Upsample from 4x4 -> 8x8\n",
        "            nn.Conv2d(feature_maps * 8, feature_maps * 4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(feature_maps * 4),\n",
        "            nn.ReLU(inplace=True),  # Non-linear activation\n",
        "\n",
        "            # Self-Attention layer for global coherence\n",
        "            SelfAttention(feature_maps * 4),\n",
        "\n",
        "            nn.Upsample(scale_factor=2),  # Upsample from 8x8 -> 16x16\n",
        "            nn.Conv2d(feature_maps * 4, feature_maps * 2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(feature_maps * 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Upsample(scale_factor=2),  # Upsample from 16x16 -> 32x32\n",
        "            nn.Conv2d(feature_maps * 2, img_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()  # Output values in the range [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # Map noise to feature space and reshape to initial image size\n",
        "        out = self.fc(z).view(z.size(0), -1, self.init_size, self.init_size)\n",
        "        return self.gen(out)  # Generate final image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RdkWvSXUrm6C"
      },
      "outputs": [],
      "source": [
        "# Step 5: Define the Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_channels, feature_maps):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # Define the discriminator network\n",
        "        self.disc = nn.Sequential(\n",
        "            nn.Conv2d(img_channels, feature_maps, kernel_size=4, stride=2, padding=1),  # 32x32 -> 16x16\n",
        "            nn.LeakyReLU(0.2, inplace=True),  # Leaky ReLU prevents \"dead neurons\"\n",
        "\n",
        "            nn.Conv2d(feature_maps, feature_maps * 2, kernel_size=4, stride=2, padding=1),  # 16x16 -> 8x8\n",
        "            nn.BatchNorm2d(feature_maps * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # Self-Attention layer to capture global dependencies\n",
        "            SelfAttention(feature_maps * 2),\n",
        "\n",
        "            nn.Conv2d(feature_maps * 2, feature_maps * 4, kernel_size=4, stride=2, padding=1),  # 8x8 -> 4x4\n",
        "            nn.BatchNorm2d(feature_maps * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "        # Fully connected layer to classify real vs fake\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(feature_maps * 4 * 4 * 4, 1),  # Flatten features to a single value\n",
        "            nn.Sigmoid()  # Output in the range [0, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Extract features using convolutional layers\n",
        "        features = self.disc(img).view(img.size(0), -1)\n",
        "        return self.fc(features)  # Return classification result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvRt0nIcrsvH",
        "outputId": "ffe66eb2-137a-4f3e-fb46-04093901d207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Disc Loss: 1.1468, Gen Loss: 1.4797\n",
            "Epoch [2/50], Disc Loss: 0.9381, Gen Loss: 1.2716\n",
            "Epoch [3/50], Disc Loss: 0.9619, Gen Loss: 1.9820\n",
            "Epoch [4/50], Disc Loss: 0.8605, Gen Loss: 1.8880\n",
            "Epoch [5/50], Disc Loss: 0.7269, Gen Loss: 1.3613\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Training Loop\n",
        "\n",
        "# Initialize models\n",
        "z_dim = 100  # Dimensionality of noise vector\n",
        "gen = Generator(z_dim, img_channels=3, feature_maps=64)  # Generator model\n",
        "disc = Discriminator(img_channels=3, feature_maps=64)  # Discriminator model\n",
        "\n",
        "# Define optimizers for both networks\n",
        "lr = 0.0002  # Learning rate for Adam optimizer\n",
        "optim_gen = optim.Adam(gen.parameters(), lr=lr, betas=(0.5, 0.999))  # Optimizer for generator\n",
        "optim_disc = optim.Adam(disc.parameters(), lr=lr, betas=(0.5, 0.999))  # Optimizer for discriminator\n",
        "\n",
        "# Binary cross-entropy loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Training loop parameters\n",
        "epochs = 50  # Number of epochs\n",
        "z_dim = 100  # Latent space dimensionality\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for real, _ in dataloader:\n",
        "        # Use CPU-friendly operations (remove `.cuda()` calls)\n",
        "        batch_size = real.size(0)  # Batch size of the current batch\n",
        "\n",
        "        # Generate random noise and fake images\n",
        "        z = torch.randn(batch_size, z_dim)  # Random noise vector\n",
        "        fake = gen(z).detach()  # Detach from generator to avoid gradient computation\n",
        "\n",
        "        # Labels for real and fake images\n",
        "        real_labels = torch.ones(batch_size, 1)  # Labels for real images\n",
        "        fake_labels = torch.zeros(batch_size, 1)  # Labels for fake images\n",
        "\n",
        "        # Train discriminator\n",
        "        disc_loss_real = criterion(disc(real), real_labels)  # Loss for real images\n",
        "        disc_loss_fake = criterion(disc(fake), fake_labels)  # Loss for fake images\n",
        "        disc_loss = disc_loss_real + disc_loss_fake  # Total discriminator loss\n",
        "\n",
        "        optim_disc.zero_grad()  # Zero out gradients for discriminator\n",
        "        disc_loss.backward()  # Backpropagate discriminator loss\n",
        "        optim_disc.step()  # Update discriminator weights\n",
        "\n",
        "        # Train generator\n",
        "        z = torch.randn(batch_size, z_dim)  # Generate new noise vector\n",
        "        fake = gen(z)  # Generate fake images\n",
        "        gen_loss = criterion(disc(fake), real_labels)  # Generator loss (fool the discriminator)\n",
        "\n",
        "        optim_gen.zero_grad()  # Zero out gradients for generator\n",
        "        gen_loss.backward()  # Backpropagate generator loss\n",
        "        optim_gen.step()  # Update generator weights\n",
        "\n",
        "    # Print progress for each epoch\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Disc Loss: {disc_loss.item():.4f}, Gen Loss: {gen_loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erCAIr6Lr9cM"
      },
      "outputs": [],
      "source": [
        "# Step 7 : Generate and visualize images\n",
        "# Generate random samples\n",
        "z = torch.randn(16, z_dim).cuda()\n",
        "samples = gen(z).detach().cpu()  # Detach from computation graph for visualization\n",
        "samples = (samples + 1) / 2  # Rescale images to [0, 1]\n",
        "\n",
        "# Create a grid of images\n",
        "grid = utils.make_grid(samples, nrow=4)\n",
        "plt.imshow(grid.permute(1, 2, 0))  # Convert to (H, W, C) for visualization\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKQhXQDDsQVv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
