{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Import Necessary Libraries\n"
      ],
      "metadata": {
        "id": "wb83cSxJy2bN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RsCzRe9dyhhg"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import defaultdict\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Define Functions for BPE\n"
      ],
      "metadata": {
        "id": "TD8gXI3Qy6kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1: Function to Build Initial Vocabulary\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    vocab = defaultdict(int)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Add start and end tokens to the sentence\n",
        "        sentence = '<s> ' + sentence + ' </s>'\n",
        "        for char in sentence:\n",
        "            vocab[char] += 1\n",
        "\n",
        "    return vocab\n"
      ],
      "metadata": {
        "id": "59VfHj1gyqap"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2.2: Function to Count Pairs\n",
        "def get_stats(vocab):\n",
        "    pairs = defaultdict(int)\n",
        "\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pair = (symbols[i], symbols[i + 1])\n",
        "            pairs[pair] += freq\n",
        "\n",
        "    return pairs\n"
      ],
      "metadata": {
        "id": "tA2BzT3TyqXg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.3: Function to Merge Pairs\n",
        "\n",
        "def merge_vocab(pair, vocab):\n",
        "    new_vocab = {}\n",
        "    bigram = ' '.join(pair)\n",
        "\n",
        "    replacement = ''.join(pair)\n",
        "    for word in vocab:\n",
        "        new_word = word.replace(bigram, replacement)\n",
        "        new_vocab[new_word] = vocab[word]\n",
        "\n",
        "    return new_vocab\n"
      ],
      "metadata": {
        "id": "btvw5Dr-yqUR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3: Main BPE Algorithm\n"
      ],
      "metadata": {
        "id": "PwVXzBMdzFZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bpe(sentences, num_merges):\n",
        "    # Step 1: Build initial vocabulary\n",
        "    vocab = build_vocab(sentences)\n",
        "\n",
        "    # Step 2: Perform merges\n",
        "    for _ in range(num_merges):\n",
        "        pairs = get_stats(vocab)\n",
        "\n",
        "        # If no pairs left, break the loop\n",
        "        if not pairs:\n",
        "            break\n",
        "\n",
        "        # Find the most frequent pair\n",
        "        best_pair = max(pairs, key=pairs.get)\n",
        "\n",
        "        # Merge the best pair in the vocabulary\n",
        "        vocab = merge_vocab(best_pair, vocab)\n",
        "\n",
        "    return vocab\n"
      ],
      "metadata": {
        "id": "TO8yW-mfyqRM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Example Usage\n"
      ],
      "metadata": {
        "id": "jdX_T-SFzIYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sentences for demonstration\n",
        "sentences = [\n",
        "    \"hello there\",\n",
        "    \"hello world\",\n",
        "    \"hello\",\n",
        "    \"there world\"\n",
        "]\n",
        "\n",
        "# Specify the number of merges\n",
        "num_merges = 5\n",
        "\n",
        "# Apply BPE\n",
        "final_vocab = bpe(sentences, num_merges)\n",
        "\n",
        "# Print the final vocabulary\n",
        "print(\"Final Vocabulary:\")\n",
        "for word, freq in final_vocab.items():\n",
        "    print(f\"{word}: {freq}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_F-lKh4yqN5",
        "outputId": "e6cac4f0-40b2-4b6d-f441-31a03eebb9bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Vocabulary:\n",
            "<: 8\n",
            "s: 8\n",
            ">: 8\n",
            " : 11\n",
            "h: 5\n",
            "e: 7\n",
            "l: 8\n",
            "o: 5\n",
            "t: 2\n",
            "r: 4\n",
            "/: 4\n",
            "w: 2\n",
            "d: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BPETokenizer(tf.keras.preprocessing.text.Tokenizer):\n",
        "    def __init__(self, num_merges):\n",
        "        super(BPETokenizer, self).__init__()\n",
        "        self.num_merges = num_merges\n",
        "\n",
        "    def fit_on_texts(self, texts):\n",
        "        self.vocab = bpe(texts, self.num_merges)\n",
        "\n",
        "    def texts_to_sequences(self, texts):\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            tokens = []\n",
        "            for word in text.split():\n",
        "                if word in self.vocab:\n",
        "                    tokens.append(word)\n",
        "                else:\n",
        "                    tokens.append(\"<UNK>\")  # handle OOV words\n",
        "            sequences.append(tokens)\n",
        "        return sequences\n"
      ],
      "metadata": {
        "id": "mDA9-rGnyqKp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Integrating with TensorFlow\n"
      ],
      "metadata": {
        "id": "ZbnFNY1nzMIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1: Create a Tokenizer\n",
        "\n",
        "class BPETokenizer(tf.keras.preprocessing.text.Tokenizer):\n",
        "    def __init__(self, num_merges):\n",
        "        super(BPETokenizer, self).__init__()\n",
        "        self.num_merges = num_merges\n",
        "\n",
        "    def fit_on_texts(self, texts):\n",
        "        self.vocab = bpe(texts, self.num_merges)\n",
        "\n",
        "    def texts_to_sequences(self, texts):\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            tokens = []\n",
        "            for word in text.split():\n",
        "                if word in self.vocab:\n",
        "                    tokens.append(word)\n",
        "                else:\n",
        "                    tokens.append(\"<UNK>\")  # handle OOV words\n",
        "            sequences.append(tokens)\n",
        "        return sequences\n"
      ],
      "metadata": {
        "id": "7vF91KuMyy4I"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Example Usage of the Tokenizer\n"
      ],
      "metadata": {
        "id": "TEbnSjNRzaHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the BPE tokenizer\n",
        "bpe_tokenizer = BPETokenizer(num_merges=5)\n",
        "\n",
        "# Fit the tokenizer on the sample sentences\n",
        "bpe_tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Convert texts to sequences\n",
        "sequences = bpe_tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Print the sequences\n",
        "print(\"Tokenized Sequences:\")\n",
        "for seq in sequences:\n",
        "    print(seq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQvIhz4uzXLC",
        "outputId": "576ff055-6771-4416-8eb7-6006f03c52ad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Sequences:\n",
            "['<UNK>', '<UNK>']\n",
            "['<UNK>', '<UNK>']\n",
            "['<UNK>']\n",
            "['<UNK>', '<UNK>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dsaDOQm1zXU1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}