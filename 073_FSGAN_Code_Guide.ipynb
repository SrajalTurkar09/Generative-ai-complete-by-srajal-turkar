{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d622f4",
   "metadata": {},
   "source": [
    "# Face-Swap GAN (FSGAN) Implementation Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34dea20",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef255d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dlib\n",
      "  Using cached dlib-19.24.6.tar.gz (3.4 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: dlib\n",
      "  Building wheel for dlib (setup.py): started\n",
      "  Building wheel for dlib (setup.py): still running...\n",
      "  Building wheel for dlib (setup.py): still running...\n",
      "  Building wheel for dlib (setup.py): still running...\n",
      "  Building wheel for dlib (setup.py): still running...\n",
      "  Building wheel for dlib (setup.py): finished with status 'done'\n",
      "  Created wheel for dlib: filename=dlib-19.24.6-cp39-cp39-win_amd64.whl size=3069725 sha256=f90000113d4d84fd288c4535328e0eba3c5f93a437ced026cd534902276dfe79\n",
      "  Stored in directory: c:\\users\\rishu\\appdata\\local\\pip\\cache\\wheels\\f5\\ba\\ea\\ad13d4a963e5dbbbaf4dce5fef93353e2e7586ba8b7787221c\n",
      "Successfully built dlib\n",
      "Installing collected packages: dlib\n",
      "Successfully installed dlib-19.24.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\rishu\\appdata\\local\\anaconda3\\envs\\rexxes\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\rishu\\appdata\\local\\anaconda3\\envs\\rexxes\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\rishu\\appdata\\local\\anaconda3\\envs\\rexxes\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install dlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow opencv-python dlib numpy matplotlib tqdm --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da09097a",
   "metadata": {},
   "source": [
    "\n",
    "- **TensorFlow**: For building the GAN model.\n",
    "- **OpenCV**: For image processing tasks.\n",
    "- **dlib**: For facial landmark detection.\n",
    "- **NumPy**: For numerical computations.\n",
    "- **Matplotlib**: For visualizing results.\n",
    "- **tqdm**: For progress tracking during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73670ec7",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8166b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing necessary libraries\n",
    "import tensorflow as tf  # TensorFlow for creating deep learning models\n",
    "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, UpSampling2D, Concatenate, Dense, Flatten  # Essential Keras layers\n",
    "from tensorflow.keras.models import Model  # To define and compile models\n",
    "import cv2  # OpenCV for image processing and face alignment\n",
    "import dlib  # For facial landmark detection\n",
    "import numpy as np  # NumPy for numerical operations\n",
    "import matplotlib.pyplot as plt  # Matplotlib for visualizing data\n",
    "from tqdm import tqdm  # Tqdm for showing progress bars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4a1f8",
   "metadata": {},
   "source": [
    "\n",
    "- TensorFlow and Keras: For constructing and training the GAN.\n",
    "- OpenCV (`cv2`) and dlib: For face detection and preprocessing.\n",
    "- NumPy: To handle image arrays.\n",
    "- Matplotlib: To visualize image outputs.\n",
    "- tqdm: Displays progress bars during loops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3282c2",
   "metadata": {},
   "source": [
    "## 3. Load and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed66d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing dlib's face detector and shape predictor\n",
    "detector = dlib.get_frontal_face_detector()  # Detects frontal faces in an image\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  # Predicts 68 landmarks for face alignment\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(image_path, img_size=128):\n",
    "    \"\"\"\n",
    "   \n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)  # Read the image from the file path\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR (OpenCV format) to RGB\n",
    "    \n",
    "    faces = detector(img)  # Detect faces in the image\n",
    "    if len(faces) == 0:  # If no faces are detected, raise an error\n",
    "        raise ValueError(\"No face detected!\")\n",
    "    \n",
    "    for face in faces:  # Iterate over detected faces (assumes one face per image)\n",
    "        landmarks = predictor(img, face)  # Predict facial landmarks\n",
    "        aligned_face = align_face(img, landmarks)  # Align face based on detected landmarks (function to define)\n",
    "        resized_face = cv2.resize(aligned_face, (img_size, img_size))  # Resize the aligned face to the desired size\n",
    "        return resized_face / 255.0  # Normalize pixel values to [0, 1] for GAN input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0fe62b",
   "metadata": {},
   "source": [
    "\n",
    "- **detector**: Detects faces in the input images.\n",
    "- **predictor**: Identifies 68 facial landmarks, essential for alignment.\n",
    "- **align_face**: A helper function to crop and align faces based on landmarks (to be defined later).\n",
    "- **Normalization**: Converts pixel values to the range [0, 1] for better GAN performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c37b7a1",
   "metadata": {},
   "source": [
    "## 4. Define the FSGAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa20d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(input_shape=(128, 128, 3)):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)  # Input layer with specified shape\n",
    "    \n",
    "    # Downsampling layers\n",
    "    x = Conv2D(64, kernel_size=4, strides=2, padding=\"same\")(inputs)  # Convolution with stride 2 to downsample\n",
    "    x = LeakyReLU(alpha=0.2)(x)  # LeakyReLU for better gradient flow during training\n",
    "    \n",
    "    # Bottleneck (adds more convolution layers for feature extraction)\n",
    "    for _ in range(4):  # Repeat 4 times to increase feature extraction depth\n",
    "        x = Conv2D(128, kernel_size=4, strides=2, padding=\"same\")(x)\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    # Upsampling layers (reconstruct the image from bottleneck features)\n",
    "    for _ in range(4):  # Repeat 4 times to scale up the spatial dimensions\n",
    "        x = UpSampling2D(size=2)(x)  # Upsample by a factor of 2\n",
    "        x = Conv2D(128, kernel_size=4, padding=\"same\")(x)  # Convolution after upsampling\n",
    "        x = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    outputs = Conv2D(3, kernel_size=7, activation=\"tanh\", padding=\"same\")(x)  # Output layer with tanh activation\n",
    "    return Model(inputs, outputs, name=\"Generator\")  # Return the complete generator model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef77cfc",
   "metadata": {},
   "source": [
    "- **Input**: Takes in a 128x128x3 image.\n",
    "- **Downsampling**: Extracts features while reducing spatial dimensions.\n",
    "- **Bottleneck**: Encodes image features into a compact representation.\n",
    "- **Upsampling**: Reconstructs the output image to the original resolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "910862d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(input_shape=(128, 128, 3)):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)  # Input layer with specified shape\n",
    "    \n",
    "    # Downsampling layers\n",
    "    x = Conv2D(64, kernel_size=4, strides=2, padding=\"same\")(inputs)  # Convolution with stride 2 to downsample\n",
    "    x = LeakyReLU(alpha=0.2)(x)  # LeakyReLU activation for gradient stability\n",
    "    \n",
    "    for _ in range(3):  # Add additional convolutional layers\n",
    "        x = Conv2D(128, kernel_size=4, strides=2, padding=\"same\")(x)  # Downsampling further\n",
    "        x = LeakyReLU(alpha=0.2)(x)  # Activation function for non-linearity\n",
    "    \n",
    "    # Flattening and output layer\n",
    "    x = Flatten()(x)  # Flatten the features into a single vector\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)  # Output a single probability (real or fake)\n",
    "    \n",
    "    return Model(inputs, x, name=\"Discriminator\")  # Return the complete discriminator model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec2ba7",
   "metadata": {},
   "source": [
    "\n",
    "- **Conv2D Layers**: Extract features at multiple levels.\n",
    "- **LeakyReLU**: Handles negative activations, aiding in stable GAN training.\n",
    "- **Dense Layer**: Produces a single output indicating real or fake.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a2e446",
   "metadata": {},
   "source": [
    "## 5. Compile the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa2f8967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_fsgan(generator, discriminator):\n",
    "    discriminator.compile(optimizer=tf.keras.optimizers.Adam(0.0002), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    generated_image = generator(inputs)\n",
    "    valid = discriminator(generated_image)\n",
    "    \n",
    "    gan_model = Model(inputs, valid)\n",
    "    gan_model.compile(optimizer=tf.keras.optimizers.Adam(0.0002), loss=\"binary_crossentropy\")\n",
    "    return gan_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfa8314",
   "metadata": {},
   "source": [
    "\n",
    "- **Generator**: Learns to create realistic face-swapped images.\n",
    "- **Discriminator**: Differentiates real from fake images.\n",
    "- **GAN Model**: Combines both networks for joint training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edafb52c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9d58465",
   "metadata": {},
   "source": [
    "# 6. Train the FSGAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d61aed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fsgan(generator, discriminator, gan_model, epochs, batch_size, dataset):\n",
    "    for epoch in range(epochs):\n",
    "        for i in tqdm(range(len(dataset) // batch_size)):\n",
    "            real_images = dataset[i * batch_size:(i + 1) * batch_size]\n",
    "            noise = np.random.normal(0, 1, (batch_size, 128, 128, 3))\n",
    "            fake_images = generator.predict(noise)\n",
    "            \n",
    "            # Train discriminator\n",
    "            real_labels = np.ones((batch_size, 1))\n",
    "            fake_labels = np.zeros((batch_size, 1))\n",
    "            d_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
    "            d_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
    "            \n",
    "            # Train generator\n",
    "            g_loss = gan_model.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | D Loss: {d_loss_real[0]:.4f}, G Loss: {g_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87de1cb9",
   "metadata": {},
   "source": [
    "\n",
    "- **Real and Fake Labels**: Used for supervised training of the discriminator.\n",
    "- **GAN Loss**: Encourages the generator to create realistic images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738ad1d",
   "metadata": {},
   "source": [
    "# 7. Test and Visualize Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e152580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fsgan(generator, input_image):\n",
    "    generated_image = generator.predict(np.expand_dims(input_image, axis=0))[0]\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(input_image)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Face-Swapped Image\")\n",
    "    plt.imshow((generated_image + 1) / 2)  # De-normalize\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54de53b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"Discriminator\" is incompatible with the layer: expected shape=(None, 128, 128, 3), found shape=(None, 64, 64, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m generator \u001b[38;5;241m=\u001b[39m build_generator()\n\u001b[0;32m      3\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m build_discriminator()\n\u001b[1;32m----> 4\u001b[0m gan_model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_fsgan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m train_fsgan(generator, discriminator, gan_model, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, dataset\u001b[38;5;241m=\u001b[39mdataset)\n\u001b[0;32m      6\u001b[0m test_fsgan(generator, input_image)\n",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m, in \u001b[0;36mbuild_fsgan\u001b[1;34m(generator, discriminator)\u001b[0m\n\u001b[0;32m      5\u001b[0m inputs \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m      6\u001b[0m generated_image \u001b[38;5;241m=\u001b[39m generator(inputs)\n\u001b[1;32m----> 7\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m gan_model \u001b[38;5;241m=\u001b[39m Model(inputs, valid)\n\u001b[0;32m     10\u001b[0m gan_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;241m0.0002\u001b[39m), loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"Discriminator\" is incompatible with the layer: expected shape=(None, 128, 128, 3), found shape=(None, 64, 64, 3)"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "generator = build_generator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the pre-trained FSGAN model (assuming you've trained or downloaded it)\n",
    "model = load_model('path_to_your_trained_model.h5')\n",
    "\n",
    "# Load your images (make sure to have two face images for swapping)\n",
    "image1 = cv2.imread('path_to_image1.jpg')\n",
    "image2 = cv2.imread('path_to_image2.jpg')\n",
    "\n",
    "# Pre-process the images (resize and normalize as required by the model)\n",
    "image1_resized = cv2.resize(image1, (256, 256))  # Adjust to your model's input size\n",
    "image2_resized = cv2.resize(image2, (256, 256))\n",
    "\n",
    "image1_normalized = image1_resized / 255.0\n",
    "image2_normalized = image2_resized / 255.0\n",
    "\n",
    "# Concatenate the images (or adjust according to your model input)\n",
    "input_images = np.concatenate([image1_normalized, image2_normalized], axis=0)\n",
    "input_images = np.expand_dims(input_images, axis=0)\n",
    "\n",
    "# Generate the swapped face\n",
    "output = model.predict(input_images)\n",
    "\n",
    "# Post-process the output if needed (for example, denormalize the images)\n",
    "output_image = output[0] * 255.0  # Assuming the model outputs in [0, 1] range\n",
    "output_image = output_image.astype(np.uint8)\n",
    "\n",
    "# Display the results\n",
    "plt.imshow(output_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rexxes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
