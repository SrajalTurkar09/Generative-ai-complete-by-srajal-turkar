{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96859510",
   "metadata": {},
   "source": [
    "# VAE with Normalizing Flows: Theory Explanation\n",
    "\n",
    "Variational Autoencoders (VAE) and Normalizing Flows (NF) are both powerful techniques in generative modeling, each with its own strengths. Combining them allows us to significantly improve the flexibility of the latent space in a VAE.\n",
    "\n",
    "## Variational Autoencoders (VAE):\n",
    "VAE is a type of generative model designed to learn the underlying distribution of the data. It works by learning a probabilistic mapping from the input data to a latent space (lower-dimensional space) and then reconstructing the input data from the latent variables.\n",
    "\n",
    "### Key components of a VAE:\n",
    "- **Encoder**: Maps input data to a probabilistic distribution over the latent space (mean and variance).\n",
    "- **Decoder**: Reconstructs the data from the latent variable using the learned distribution.\n",
    "- **Reparameterization Trick**: To backpropagate through the stochastic sampling process in VAEs, we use a reparameterization trick that allows gradients to flow through the latent space.\n",
    "\n",
    "### Loss Function in VAE:\n",
    "The VAE loss consists of two components:\n",
    "- **Reconstruction Loss** (e.g., MSE or Binary Cross-Entropy): Measures how well the decoder reconstructs the input data from the latent representation.\n",
    "- **KL Divergence**: Regularizes the latent space by measuring the difference between the learned distribution (from the encoder) and the prior distribution (usually a standard Gaussian).\n",
    "\n",
    "## Normalizing Flows (NF):\n",
    "Normalizing Flows are a family of generative models that apply a sequence of invertible transformations to a simple distribution (e.g., a Gaussian) to transform it into a more complex distribution. The key idea is that these transformations allow us to model more complex distributions by ensuring that the Jacobian (determinant of the derivative) of the transformation is tractable.\n",
    "\n",
    "By using invertible transformations, we can sample from the transformed distribution and also compute the log-likelihood of the samples efficiently.\n",
    "\n",
    "### How Normalizing Flows work:\n",
    "- **Base Distribution**: Start with a simple base distribution (e.g., Gaussian).\n",
    "- **Transformation**: Apply a series of invertible transformations that progressively map the base distribution to a more complex one.\n",
    "- **Log-Determinant**: For each transformation, we compute the log-determinant of the Jacobian, which is required to calculate the likelihood of the transformed samples.\n",
    "\n",
    "## Combining VAEs with Normalizing Flows:\n",
    "The combination of VAEs and normalizing flows allows us to improve the flexibility of the latent space. Normalizing flows act as an additional layer on top of the VAE, providing more expressive power to the latent variable distribution. This helps model more complex data distributions that a standard VAE might struggle with.\n",
    "\n",
    "Hereâ€™s how the combination works:\n",
    "- **VAE Encoder**: The encoder maps the input to a distribution in the latent space (mean and log variance).\n",
    "- **Normalizing Flows**: After sampling the latent variable from the VAE's encoder, normalizing flows transform it into a more flexible distribution, allowing for more complex data generation.\n",
    "- **VAE Decoder**: The decoder reconstructs the data from the transformed latent variables.\n",
    "\n",
    "The loss function is modified to include the log-determinant of the transformations applied by the normalizing flows, which ensures that the model accounts for the complexity of the latent distribution.\n",
    "\n",
    "## Training Process:\n",
    "### Forward Pass:\n",
    "1. The input data is passed through the encoder to obtain the mean and log variance of the latent distribution.\n",
    "2. Samples are drawn from this distribution using the reparameterization trick.\n",
    "3. Normalizing flows are applied to the sampled latent variable.\n",
    "4. The decoder reconstructs the input data from the transformed latent variable.\n",
    "\n",
    "### Loss Calculation:\n",
    "- **Reconstruction Loss**: Measures how accurately the decoder reconstructs the input.\n",
    "- **KL Divergence**: Regularizes the latent space, encouraging it to be close to a standard Gaussian.\n",
    "- **Log-Determinant of the Jacobian**: The flow transformation introduces a Jacobian term that needs to be added to the loss to account for the complexity of the transformation.\n",
    "\n",
    "### Optimization:\n",
    "- The model parameters (encoder, decoder, and flow layers) are optimized using gradient descent to minimize the overall loss.\n",
    "\n",
    "## Key Takeaways:\n",
    "- **Normalizing Flows** improve the flexibility of the latent space by using invertible transformations, allowing for more complex data distributions.\n",
    "- Combining VAEs and normalizing flows enhances the VAE's ability to model complex data distributions.\n",
    "- The loss function in a VAE with normalizing flows includes the reconstruction loss, KL divergence, and log-determinant of the flow transformation.\n",
    "- **Normalizing flows** enable expressive latent variable models that can generate high-quality data, even from complex distributions.\n",
    "\n",
    "By integrating normalizing flows into VAEs, we achieve more accurate generative models capable of learning and generating complex data structures like images, text, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9c63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install tensorflow_probability matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers, Model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c60387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a time-series dataset (e.g., temperature data)\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\"\n",
    "data = pd.read_csv(url)\n",
    "# Extract time-series data\n",
    "data_series = data['Passengers'].values.astype(np.float32)\n",
    "# Normalize the data\n",
    "data_series = (data_series - np.mean(data_series)) / np.std(data_series)\n",
    "# Split into train and test sets\n",
    "train_data = data_series[:100]\n",
    "test_data = data_series[100:]\n",
    "# Create sliding windows for time-series forecasting\n",
    "def create_windows(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i + window_size])\n",
    "        y.append(data[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "window_size = 10\n",
    "X_train, y_train = create_windows(train_data, window_size)\n",
    "X_test, y_test = create_windows(test_data, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f667b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GP-VAE components\n",
    "tfd = tfp.distributions\n",
    "\n",
    "class GPVAE(Model):\n",
    "    def __init__(self, latent_dim, time_steps, kernel_scale=1.0):\n",
    "        super(GPVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.time_steps = time_steps\n",
    "        self.kernel_scale = kernel_scale\n",
    "        # Encoder: Maps inputs to latent space\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(time_steps,)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(latent_dim * 2)  # Mean and log-variance\n",
    "        ])\n",
    "        # Decoder: Reconstructs data from latent space\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(latent_dim,)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(time_steps)  # Reconstructs time series\n",
    "        ])\n",
    "    def call(self, inputs):\n",
    "        # Encode inputs\n",
    "        encoded = self.encoder(inputs)\n",
    "        z_mean, z_logvar = tf.split(encoded, num_or_size_splits=2, axis=-1)\n",
    "        z_std = tf.exp(0.5 * z_logvar)\n",
    "        # Sample latent variable z\n",
    "        eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        z = z_mean + eps * z_std\n",
    "        # GP prior on z (time-series dependency)\n",
    "gp_kernel = tfp.math.psd_kernels.ExponentiatedQuadratic(amplitude=self.kernel_scale)\n",
    "gp_prior = tfd.GaussianProcess(kernel=gp_kernel, index_points=tf.range(self.time_steps, dtype=tf.float32)[:, None])\n",
    "        # KL divergence between latent z and GP prior\n",
    "        kl_divergence = tfd.kl_divergence(\n",
    "            tfd.MultivariateNormalDiag(loc=z_mean, scale_diag=z_std),\n",
    "            gp_prior\n",
    "        )\n",
    "        # Decode reconstructed sequence\n",
    "        reconstruction = self.decoder(z)\n",
    "        return reconstruction, kl_divergence\n",
    "    def compute_loss(self, x):\n",
    "        reconstruction, kl_divergence = self(x)\n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = tf.reduce_mean(tf.square(x - reconstruction))\n",
    "        # Total loss = reconstruction + KL divergence\n",
    "        return reconstruction_loss + tf.reduce_mean(kl_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65812337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "latent_dim = 5\n",
    "time_steps = window_size\n",
    "gp_vae = GPVAE(latent_dim=latent_dim, time_steps=time_steps)\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = gp_vae.compute_loss(X_train)\n",
    "    gradients = tape.gradient(loss, gp_vae.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, gp_vae.trainable_variables))\n",
    "    # Print progress\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a9becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "reconstructions, _ = gp_vae(X_test)\n",
    "# Plot predictions vs actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test, label=\"True\")\n",
    "plt.plot(reconstructions.numpy()[:, -1], label=\"Predicted\")\n",
    "plt.fill_between(\n",
    "    range(len(y_test)),\n",
    "    reconstructions.numpy()[:, -1] - 0.1,\n",
    "    reconstructions.numpy()[:, -1] + 0.1,\n",
    "    color='gray', alpha=0.3, label='Uncertainty'\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(\"GP-VAE: Time-Series Forecasting with Uncertainty\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
