{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1e479f71",
      "metadata": {
        "id": "1e479f71"
      },
      "source": [
        "# Ladder Variational Autoencoder (Ladder VAE) Implementation\n",
        "This notebook demonstrates the implementation of a Ladder VAE using the CIFAR-10 dataset. It includes the following steps:\n",
        "\n",
        "1. Data Preparation\n",
        "2. Encoder Definition\n",
        "3. Decoder Definition\n",
        "4. Ladder VAE Model\n",
        "5. Training\n",
        "6. Evaluation and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "df709cc1",
      "metadata": {
        "id": "df709cc1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65bbf629",
      "metadata": {
        "id": "65bbf629"
      },
      "source": [
        "## Step 1: Load and Preprocess CIFAR-10 Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2fc2c1f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fc2c1f1",
        "outputId": "0bf91eb5-127a-43cf-a8a2-ecd8d1b066cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 0us/step\n",
            "Training data shape: (50000, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(x_train, _), (x_test, _) = cifar10.load_data()\n",
        "\n",
        "# Normalize images to the range [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Input dimensions\n",
        "input_dim = x_train.shape[1:]  # (32, 32, 3)\n",
        "print(\"Training data shape:\", x_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad8a66fc",
      "metadata": {
        "id": "ad8a66fc"
      },
      "source": [
        "## Step 2: Define the Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f59f9047",
      "metadata": {
        "id": "f59f9047"
      },
      "outputs": [],
      "source": [
        "class Encoder(Model):\n",
        "    def __init__(self, latent_dims):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv_layers = [\n",
        "            layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "        ]\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.dense_mu = [layers.Dense(dim) for dim in latent_dims]\n",
        "        self.dense_log_var = [layers.Dense(dim) for dim in latent_dims]\n",
        "\n",
        "    def call(self, x):\n",
        "        for conv in self.conv_layers:\n",
        "            x = conv(x)\n",
        "        x = self.flatten(x)\n",
        "        mu, log_var = [], []\n",
        "        for dense_mu, dense_log_var in zip(self.dense_mu, self.dense_log_var):\n",
        "            mu.append(dense_mu(x))\n",
        "            log_var.append(dense_log_var(x))\n",
        "        return mu, log_var"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f513ec7",
      "metadata": {
        "id": "7f513ec7"
      },
      "source": [
        "## Step 3: Define the Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d0cc00e2",
      "metadata": {
        "id": "d0cc00e2"
      },
      "outputs": [],
      "source": [
        "class Decoder(Model):\n",
        "    def __init__(self, latent_dims, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dense_layers = [\n",
        "            layers.Dense(8 * 8 * 64, activation='relu'),\n",
        "            layers.Reshape((8, 8, 64)),\n",
        "        ]\n",
        "        self.deconv_layers = [\n",
        "            layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same'),\n",
        "            layers.UpSampling2D((2, 2)),\n",
        "            layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same'),\n",
        "            layers.UpSampling2D((2, 2)),\n",
        "            layers.Conv2DTranspose(output_dim[-1], (3, 3), activation='sigmoid', padding='same'),\n",
        "        ]\n",
        "\n",
        "    def call(self, z):\n",
        "        x = z[0]  # Start with the top latent variable\n",
        "        for dense in self.dense_layers:\n",
        "            x = dense(x)\n",
        "        for deconv in self.deconv_layers:\n",
        "            x = deconv(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eebb49ec",
      "metadata": {
        "id": "eebb49ec"
      },
      "source": [
        "## Step 4: Define the Ladder VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "02b5c841",
      "metadata": {
        "id": "02b5c841"
      },
      "outputs": [],
      "source": [
        "class LadderVAE(Model):\n",
        "    def __init__(self, encoder, decoder, latent_dims):\n",
        "        super(LadderVAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.latent_dims = latent_dims\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        eps = tf.random.normal(shape=tf.shape(mu))\n",
        "        return mu + tf.exp(0.5 * log_var) * eps\n",
        "\n",
        "    def call(self, x):\n",
        "        mu, log_var = self.encoder(x)\n",
        "        z = [self.reparameterize(m, lv) for m, lv in zip(mu, log_var)]\n",
        "        reconstruction = self.decoder(z)\n",
        "        return reconstruction, mu, log_var\n",
        "\n",
        "    # def compute_loss(self, x):\n",
        "    #     reconstruction, mu, log_var = self.call(x)\n",
        "    #     recon_loss = tf.reduce_mean(tf.reduce_sum(tf.keras.losses.binary_crossentropy(x, reconstruction), axis=(1, 2, 3)))\n",
        "    #     kl_loss = sum([tf.reduce_mean(-0.5 * tf.reduce_sum(1 + lv - tf.square(m) - tf.exp(lv), axis=1)) for m, lv in zip(mu, log_var)])\n",
        "    #     return recon_loss + kl_loss\n",
        "\n",
        "    def compute_loss(self, x):\n",
        "        reconstruction, mu, log_var = self.call(x)\n",
        "        # The change is in the line below: axis is changed to (1, 2)\n",
        "        recon_loss = tf.reduce_mean(tf.reduce_sum(tf.keras.losses.binary_crossentropy(x, reconstruction), axis=(1, 2)))\n",
        "        kl_loss = sum([tf.reduce_mean(-0.5 * tf.reduce_sum(1 + lv - tf.square(m) - tf.exp(lv), axis=1)) for m, lv in zip(mu, log_var)])\n",
        "        return recon_loss + kl_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66ed7897",
      "metadata": {
        "id": "66ed7897"
      },
      "source": [
        "## Step 5: Train the Ladder VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "19ee9c3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19ee9c3a",
        "outputId": "11edd7b8-254b-4962-fa74-0a4e79509396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 651.5876\n",
            "Epoch 2, Loss: 632.0425\n",
            "Epoch 3, Loss: 629.5007\n",
            "Epoch 4, Loss: 628.2859\n",
            "Epoch 5, Loss: 627.7608\n",
            "Epoch 6, Loss: 627.3884\n",
            "Epoch 7, Loss: 627.1360\n",
            "Epoch 8, Loss: 626.9698\n",
            "Epoch 9, Loss: 626.7973\n",
            "Epoch 10, Loss: 626.6245\n",
            "Epoch 11, Loss: 626.4365\n",
            "Epoch 12, Loss: 626.3504\n",
            "Epoch 13, Loss: 626.2387\n",
            "Epoch 14, Loss: 626.0740\n",
            "Epoch 15, Loss: 626.0377\n",
            "Epoch 16, Loss: 626.0247\n",
            "Epoch 17, Loss: 625.9297\n",
            "Epoch 18, Loss: 625.8511\n",
            "Epoch 19, Loss: 625.8021\n",
            "Epoch 20, Loss: 625.7709\n"
          ]
        }
      ],
      "source": [
        "latent_dims = [128, 64, 32]\n",
        "encoder = Encoder(latent_dims)\n",
        "decoder = Decoder(latent_dims, input_dim)\n",
        "ladder_vae = LadderVAE(encoder, decoder, latent_dims)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(10000).batch(batch_size)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch in train_dataset:\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = ladder_vae.compute_loss(batch)\n",
        "        grads = tape.gradient(loss, ladder_vae.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, ladder_vae.trainable_weights))\n",
        "        epoch_loss += loss\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(train_dataset):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8d3683e",
      "metadata": {
        "id": "a8d3683e"
      },
      "source": [
        "## Step 6: Evaluate and Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2df43bd1",
      "metadata": {
        "id": "2df43bd1"
      },
      "outputs": [],
      "source": [
        "def plot_reconstructions(model, x):\n",
        "    reconstructions, _, _ = model.call(x)\n",
        "    fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
        "    for i in range(10):\n",
        "        axes[0, i].imshow(x[i])\n",
        "        axes[1, i].imshow(reconstructions[i].numpy())\n",
        "    plt.show()\n",
        "\n",
        "plot_reconstructions(ladder_vae, x_test[:10])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}