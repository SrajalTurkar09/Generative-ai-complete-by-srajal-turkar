{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ada1f2c0",
   "metadata": {},
   "source": [
    "# Recurrent GAN (RGAN) for Sequential Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c5d53",
   "metadata": {},
   "source": [
    "This notebook demonstrates the implementation of a Recurrent GAN (RGAN) to generate sequential data using the Human Activity Recognition (HAR) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48e18a2",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f472d4b",
   "metadata": {},
   "source": [
    "We use the HAR dataset, a public time-series dataset containing sensor data from wearable devices. The data is normalized and reshaped to fit the input format required by the RGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99f5f44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already extracted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train before reshaping: (7352, 561)\n",
      "Dataset shape after reshaping: (3553, 128, 9)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Function to extract, load, and preprocess the HAR dataset\n",
    "def load_har_dataset(zip_path):\n",
    "    # Specify the directory to extract the dataset\n",
    "    extract_dir = \"data/UCI HAR Dataset/\"\n",
    "    \n",
    "    # Extract the ZIP file if not already extracted\n",
    "    if not os.path.exists(extract_dir):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"data/\")\n",
    "        print(\"Dataset extracted successfully.\")\n",
    "    else:\n",
    "        print(\"Dataset already extracted.\")\n",
    "    \n",
    "    # Load training data: Each row corresponds to a sample with sequential features\n",
    "    X_train = np.loadtxt(extract_dir + \"train/X_train.txt\")\n",
    "    \n",
    "    # Print the shape before reshaping\n",
    "    print(f\"Shape of X_train before reshaping: {X_train.shape}\")\n",
    "    \n",
    "    # Normalize the data to the range [0, 1] for better training performance\n",
    "    X_train = X_train / np.max(X_train, axis=0)\n",
    "    \n",
    "    # Calculate the total number of rows that can form sequences of 128 steps\n",
    "    valid_rows = (X_train.shape[0] // 128) * 128\n",
    "    \n",
    "    # Truncate the data to ensure divisibility by 128\n",
    "    X_train = X_train[:valid_rows]\n",
    "    \n",
    "    # Reshape the truncated data into [samples, time steps, features]\n",
    "    X_train = X_train.reshape((-1, 128, 9))\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "# Path to your ZIP file\n",
    "zip_path = \"data/UCI HAR Dataset.zip\"\n",
    "\n",
    "# Call the function to load and preprocess the data\n",
    "data = load_har_dataset(zip_path)\n",
    "\n",
    "# Create a TensorFlow dataset object for training\n",
    "# Shuffle the data and create batches for training\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data).shuffle(1000).batch(batch_size)\n",
    "\n",
    "# Print the shape of the dataset to confirm dimensions\n",
    "print(f\"Dataset shape after reshaping: {data.shape}\")\n",
    "# Dataset shape: (Number of samples, Time steps, Features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1d208a",
   "metadata": {},
   "source": [
    "## 2. Define the Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cab500",
   "metadata": {},
   "source": [
    "The generator is an LSTM-based model that takes random noise as input and generates synthetic sequences matching the characteristics of the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e20a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator model for RGAN\n",
    "class RGANGenerator(tf.keras.Model):\n",
    "    def __init__(self, noise_dim, hidden_dim, seq_len, features):\n",
    "        super(RGANGenerator, self).__init__()\n",
    "        \n",
    "        # LSTM layer to process the input noise\n",
    "        self.rnn = tf.keras.layers.LSTM(hidden_dim, return_sequences=True)\n",
    "        \n",
    "        # Dense layer to map the RNN output to the required number of features\n",
    "        self.fc = tf.keras.layers.Dense(features)\n",
    "        \n",
    "        # Store the sequence length and noise dimensions for future reference\n",
    "        self.seq_len = seq_len\n",
    "        self.noise_dim = noise_dim\n",
    "\n",
    "    def call(self, z):\n",
    "        # Forward pass through the LSTM layer\n",
    "        rnn_out = self.rnn(z)  # Shape: [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Map the output to the desired feature space\n",
    "        return self.fc(rnn_out)  # Shape: [batch_size, seq_len, features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca151cc",
   "metadata": {},
   "source": [
    "## 3. Define the Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d84a1e9",
   "metadata": {},
   "source": [
    "The discriminator is an LSTM-based model that distinguishes between real and synthetic sequences, providing feedback to improve the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1b91710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator model for RGAN\n",
    "class RGANDiscriminator(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(RGANDiscriminator, self).__init__()\n",
    "        \n",
    "        # LSTM layer to process the input sequences\n",
    "        self.rnn = tf.keras.layers.LSTM(hidden_dim, return_sequences=False)\n",
    "        \n",
    "        # Dense layer to output a single probability (real or fake)\n",
    "        self.fc = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        # Forward pass through the LSTM layer\n",
    "        rnn_out = self.rnn(x)  # Shape: [batch_size, hidden_dim]\n",
    "        \n",
    "        # Output a single probability score\n",
    "        return self.fc(rnn_out)  # Shape: [batch_size, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b70726",
   "metadata": {},
   "source": [
    "## 4. Define Loss Functions and Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d28198",
   "metadata": {},
   "source": [
    "Binary Cross-Entropy loss is used for both the generator and discriminator, and Adam optimizers are used to update the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf35be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the binary cross-entropy loss\n",
    "# This is used for both the generator and discriminator\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Define optimizers for both models\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0316fb8",
   "metadata": {},
   "source": [
    "## 5. Define the Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a7896c",
   "metadata": {},
   "source": [
    "Each training step involves generating synthetic sequences, calculating losses for both models, and updating their weights using gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cb36b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function  # Optimize execution by compiling the function\n",
    "def train_step(real_sequences):\n",
    "    # Get the batch size dynamically\n",
    "    batch_size = tf.shape(real_sequences)[0]\n",
    "\n",
    "    # Generate random noise input for the generator\n",
    "    noise = tf.random.normal([batch_size, seq_len, noise_dim])\n",
    "\n",
    "    # Use GradientTape to track operations for automatic differentiation\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generate fake sequences from noise\n",
    "        fake_sequences = generator(noise)\n",
    "\n",
    "        # Get discriminator predictions for real and fake sequences\n",
    "        real_output = discriminator(real_sequences)  # Real sequence predictions\n",
    "        fake_output = discriminator(fake_sequences)  # Fake sequence predictions\n",
    "\n",
    "        # Compute discriminator loss\n",
    "        d_loss_real = bce_loss(tf.ones_like(real_output), real_output)\n",
    "        d_loss_fake = bce_loss(tf.zeros_like(fake_output), fake_output)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        # Compute generator loss\n",
    "        g_loss = bce_loss(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "    # Compute gradients for both models\n",
    "    generator_gradients = tape.gradient(g_loss, generator.trainable_variables)\n",
    "    discriminator_gradients = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "\n",
    "    # Apply gradients to update model weights\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "\n",
    "    return g_loss, d_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef6c772",
   "metadata": {},
   "source": [
    "## 6. Train the RGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd554cc",
   "metadata": {},
   "source": [
    "The RGAN is trained over multiple epochs, processing data in batches and displaying progress after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9921bfc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Rishu\\AppData\\Local\\Temp\\ipykernel_21736\\201821176.py\", line 12, in train_step  *\n        fake_sequences = generator(noise)\n\n    NameError: name 'generator' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Loop through batches of real sequences\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m real_sequences \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;66;03m# Perform a training step\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m         g_loss, d_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_sequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Print progress at the end of each epoch\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Generator Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Discriminator Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filey7z_c9f4.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(real_sequences)\u001b[0m\n\u001b[0;32m     11\u001b[0m noise \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal, ([ag__\u001b[38;5;241m.\u001b[39mld(batch_size), ag__\u001b[38;5;241m.\u001b[39mld(seq_len), ag__\u001b[38;5;241m.\u001b[39mld(noise_dim)],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape(persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 13\u001b[0m     fake_sequences \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(generator), (ag__\u001b[38;5;241m.\u001b[39mld(noise),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     14\u001b[0m     real_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(discriminator), (ag__\u001b[38;5;241m.\u001b[39mld(real_sequences),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     15\u001b[0m     fake_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(discriminator), (ag__\u001b[38;5;241m.\u001b[39mld(fake_sequences),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mNameError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Rishu\\AppData\\Local\\Temp\\ipykernel_21736\\201821176.py\", line 12, in train_step  *\n        fake_sequences = generator(noise)\n\n    NameError: name 'generator' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "# Define the sequence length and noise dimensions\n",
    "seq_len = 128  # The length of each sequence in the HAR dataset\n",
    "noise_dim = 100  # Dimensionality of the noise vector for the generator\n",
    "epochs = 50\n",
    "\n",
    "# Loop through each epoch\n",
    "for epoch in range(epochs):\n",
    "    # Loop through batches of real sequences\n",
    "    for real_sequences in dataset:\n",
    "        # Perform a training step\n",
    "        g_loss, d_loss = train_step(real_sequences)\n",
    "    \n",
    "    # Print progress at the end of each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Generator Loss: {g_loss.numpy()}, Discriminator Loss: {d_loss.numpy()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa5b145",
   "metadata": {},
   "source": [
    "## 7. Generate and Visualize Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7fcee0",
   "metadata": {},
   "source": [
    "After training, the generator creates synthetic sequences from random noise, and the results are visualized for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1d2c9a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seq_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Generate synthetic sequences after training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m noise \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal([\u001b[38;5;241m5\u001b[39m, \u001b[43mseq_len\u001b[49m, noise_dim])  \u001b[38;5;66;03m# Generate 5 sequences\u001b[39;00m\n\u001b[0;32m      3\u001b[0m generated_sequences \u001b[38;5;241m=\u001b[39m generator(noise)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Plot the generated sequences\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'seq_len' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate synthetic sequences after training\n",
    "noise = tf.random.normal([5, seq_len, noise_dim])  # Generate 5 sequences\n",
    "generated_sequences = generator(noise).numpy()\n",
    "\n",
    "# Plot the generated sequences\n",
    "for i, seq in enumerate(generated_sequences[:3]):\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.title(f\"Generated Sequence {i+1}\")\n",
    "    for feature_idx in range(features):\n",
    "        plt.plot(seq[:, feature_idx], label=f\"Feature {feature_idx+1}\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0870092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rexxes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
