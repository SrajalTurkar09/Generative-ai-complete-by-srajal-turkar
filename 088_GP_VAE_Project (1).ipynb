{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "472f95ed",
   "metadata": {},
   "source": [
    "# Gaussian Process Variational Autoencoder (GP-VAE)\n",
    "\n",
    "A **Gaussian Process Variational Autoencoder (GP-VAE)** is a hybrid model that combines the power of **Variational Autoencoders (VAE)** with **Gaussian Processes (GP)** in the latent space to capture complex dependencies in data. This model is especially useful for applications that require modeling uncertainty and correlations, such as time-series forecasting, anomaly detection, and generative modeling.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Encoder**:\n",
    "   - The encoder maps the input data $\\\\mathbf{x}$ to a probabilistic distribution over latent variables $\\\\mathbf{z}$. It typically outputs a **mean** and **variance** which define the distribution $q(\\\\mathbf{z} | \\\\mathbf{x})$ from which latent variables are sampled.\n",
    "   - The encoder network is typically a neural network (fully connected layers) that produces these parameters based on the input data.\n",
    "\n",
    "2. **Gaussian Process in Latent Space**:\n",
    "   - Instead of assuming independent latent variables (as in traditional VAEs), GP-VAE incorporates a **Gaussian Process** prior in the latent space. This prior defines smooth correlations between latent variables. The GP prior captures the dependencies between latent variables based on a kernel function (e.g., the **RBF kernel**, also called **Exponentiated Quadratic**).\n",
    "   - The GP prior enables the latent variables to exhibit smooth transitions and dependencies over time (temporal) or space (spatial), making the model suitable for time-series data or geospatial data.\n",
    "\n",
    "3. **Decoder**:\n",
    "   - The decoder takes the latent variables $\\\\mathbf{z}$ and generates the output data $\\\\mathbf{x}$. The decoder learns how to map the latent space back to the original data space using a neural network, just like in standard VAEs.\n",
    "\n",
    "### Loss Function:\n",
    "\n",
    "The **objective** during training is to maximize the **Evidence Lower Bound (ELBO)**. The ELBO consists of two main terms:\n",
    "\n",
    "1. **Reconstruction Loss**:\n",
    "   - This term measures how well the decoder reconstructs the input data from the latent variables $\\\\mathbf{z}$.\n",
    "   - Typically, the reconstruction loss is computed as the **log-likelihood** of the data under the decoder's distribution $p(\\\\mathbf{x} | \\\\mathbf{z})$. For continuous data, this could be mean squared error (MSE), and for binary data, it might be binary cross-entropy.\n",
    "\n",
    "$$ \\\\mathcal{L}_{\\\\text{recon}} = \\\\mathbb{E}_{q(\\\\mathbf{z} | \\\\mathbf{x})} \\\\left[ \\\\log p(\\\\mathbf{x} | \\\\mathbf{z}) \\\\right] $$\n",
    "\n",
    "2. **KL Divergence**:\n",
    "   - This term ensures that the posterior $q(\\\\mathbf{z} | \\\\mathbf{x})$ approximates the GP prior $p(\\\\mathbf{z})$ and penalizes deviations from the prior distribution. It regularizes the latent space, ensuring that the latent variables follow the Gaussian Process prior.\n",
    "\n",
    "$$ \\\\mathcal{L}_{\\\\text{KL}} = D_{\\\\text{KL}} \\\\left( q(\\\\mathbf{z} | \\\\mathbf{x}) || p(\\\\mathbf{z}) \\\\right) $$\n",
    "\n",
    "The total loss combines both of these terms:\n",
    "\n",
    "$$ \\\\mathcal{L}_{\\\\text{total}} = \\\\mathcal{L}_{\\\\text{recon}} + \\\\mathcal{L}_{\\\\text{KL}} $$\n",
    "\n",
    "### Training Process:\n",
    "\n",
    "1. **Initialize the Model**:\n",
    "   - The model is initialized with random parameters for both the encoder, the decoder, and the kernel function used in the GP prior.\n",
    "\n",
    "2. **Sampling Latent Variables**:\n",
    "   - During training, the model uses the **reparameterization trick** to sample latent variables $\\\\mathbf{z}$ from the approximate posterior $q(\\\\mathbf{z} | \\\\mathbf{x})$. The reparameterization trick enables the model to backpropagate through the stochastic sampling process. This is achieved by expressing $\\\\mathbf{z}$ as:\n",
    "\n",
    "$$ \\\\mathbf{z} = \\\\mu + \\\\sigma \\\\cdot \\\\epsilon $$\n",
    "\n",
    "   where $\\\\mu$ and $\\\\sigma$ are the mean and standard deviation predicted by the encoder, and $\\\\epsilon$ is a noise term sampled from a standard normal distribution $\\\\mathcal{N}(0, 1)$.\n",
    "\n",
    "3. **Forward Pass**:\n",
    "   - The encoder computes the mean $\\\\mu$ and variance $\\\\sigma$ for the latent variables $\\\\mathbf{z}$.\n",
    "   - The latent variable $\\\\mathbf{z}$ is sampled using the reparameterization trick.\n",
    "   - The decoder takes the latent variable $\\\\mathbf{z}$ and reconstructs the data $\\\\mathbf{x}$.\n",
    "\n",
    "4. **Loss Calculation**:\n",
    "   - After the forward pass, the model calculates both the reconstruction loss (how well the decoder reconstructs the input) and the KL divergence loss (how well the latent distribution approximates the GP prior).\n",
    "   - The total loss is the sum of these two terms.\n",
    "\n",
    "5. **Optimization**:\n",
    "   - The model’s parameters (encoder, decoder, and the GP kernel) are updated by minimizing the total loss using an optimizer (typically **Adam** or **RMSprop**).\n",
    "   - Backpropagation is used to compute the gradients of the total loss with respect to the model parameters, and the optimizer updates the weights to minimize the loss.\n",
    "\n",
    "6. **Training Iterations**:\n",
    "   - This process is repeated for several epochs, with the model gradually improving its ability to reconstruct the data and capture dependencies in the latent space.\n",
    "\n",
    "7. **Final Model**:\n",
    "   - After training, the model can generate new data by sampling from the latent space and passing it through the decoder. The GP prior ensures that the latent space is smooth and that generated samples have temporal or spatial coherence.\n",
    "\n",
    "### Why Gaussian Process?\n",
    "- **Capturing Complex Dependencies**: The GP prior allows the latent variables to capture complex, non-linear dependencies across time or space. This is crucial for data where observations are correlated, such as time-series data or spatial data (e.g., geospatial modeling).\n",
    "- **Uncertainty Estimation**: The GP prior also provides uncertainty estimates. The latent variables are not just single points but are distributions, so the model can quantify uncertainty in its predictions. This is especially useful in forecasting and anomaly detection, where uncertainty plays a significant role in decision-making.\n",
    "\n",
    "### Applications:\n",
    "1. **Time-Series Forecasting**: By incorporating a GP prior in the latent space, GP-VAE captures temporal dependencies, making it ideal for predicting future values of time-series data (e.g., stock prices, weather).\n",
    "2. **Anomaly Detection**: GP-VAE’s ability to model uncertainty allows it to detect anomalies effectively. If a data point is far from the expected distribution (reconstruction error and low likelihood), the model flags it as anomalous.\n",
    "3. **Robust Image Generation**: GP-VAE can also be applied to generate images, with smooth transitions between features, benefiting from the GP prior’s ability to capture spatial correlations in the latent space.\n",
    "4. **Geospatial Modeling**: For applications like environmental data analysis, where spatial correlations are critical (e.g., pollution prediction), GP-VAE models these dependencies effectively using Gaussian Processes.\n",
    "\n",
    "### Summary:\n",
    "GP-VAE merges the flexibility of **Variational Autoencoders (VAE)** with the power of **Gaussian Processes (GP)** to model complex, non-linear relationships in data, while also modeling uncertainty. Its ability to capture smooth dependencies in latent variables makes it well-suited for tasks involving time-series forecasting, anomaly detection, and generative modeling with uncertainty quantification. The training process uses **reparameterization**, **loss minimization**, and **backpropagation** to improve the model's performance over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d687c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3_mu = nn.Linear(64, latent_dim)\n",
    "        self.fc3_logvar = nn.Linear(64, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mu = self.fc3_mu(x)\n",
    "        logvar = self.fc3_logvar(x)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = torch.relu(self.fc1(z))\n",
    "        z = torch.relu(self.fc2(z))\n",
    "        return torch.sigmoid(self.fc3(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2812ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define GP-VAE Model\n",
    "class GPVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(GPVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, input_dim)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x, mu, logvar, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66fdb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Loss Function (ELBO)\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # Reconstruction loss\n",
    "    BCE = nn.BCELoss(reduction='sum')(recon_x, x)\n",
    "    \n",
    "    # KL divergence\n",
    "    # KL(Q(z|x)||P(z)) = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    # where mu and logvar are the parameters of the approximate posterior q(z|x)\n",
    "    # and sigma^2 is the variance (exp(logvar))\n",
    "    # P(z) is the Gaussian prior, so it’s N(0, I)\n",
    "    # KL term\n",
    "    KL = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return BCE + KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f51cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create synthetic dataset (2D data for simplicity)\n",
    "X, _ = make_moons(n_samples=1000, noise=0.1)\n",
    "X = torch.tensor(X, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c5d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Initialize model, optimizer\n",
    "input_dim = X.shape[1]\n",
    "latent_dim = 2  # Latent space dimensionality\n",
    "\n",
    "model = GPVAE(input_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab552a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Training Loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    recon_x, mu, logvar, z = model(X)\n",
    "    loss = loss_function(recon_x, X, mu, logvar)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Visualizing Latent Space (2D)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mu, logvar = model.encoder(X)\n",
    "    z = model.reparameterize(mu, logvar)\n",
    "\n",
    "# Plot the latent space\n",
    "plt.scatter(z[:, 0].numpy(), z[:, 1].numpy(), alpha=0.5)\n",
    "plt.title(\"Latent Space Representation\")\n",
    "plt.xlabel(\"Latent Variable 1\")\n",
    "plt.ylabel(\"Latent Variable 2\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
