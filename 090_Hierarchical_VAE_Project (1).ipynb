{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37b21da7",
   "metadata": {},
   "source": [
    "# Hierarchical Variational Autoencoders (VAEs): A Complete Brief Explanation\n",
    "\n",
    "## 1. Introduction to VAEs:\n",
    "Variational Autoencoders (VAEs) are a class of generative models that learn to represent complex data distributions in a simpler, lower-dimensional latent space. They are composed of two main components:\n",
    "- **Encoder**: Maps data to a probabilistic latent space, typically Gaussian.\n",
    "- **Decoder**: Reconstructs the data from the latent variables.\n",
    "\n",
    "The VAE optimization consists of:\n",
    "- **Reconstruction Loss**: Measures how closely the generated data resembles the input.\n",
    "- **KL Divergence Loss**: Regularizes the latent space by encouraging the distribution of latent variables to match a prior distribution (usually Gaussian).\n",
    "\n",
    "## 2. What Are Hierarchical Priors?:\n",
    "In a standard VAE, we assume that the latent variables are independent and come from a simple prior (usually a Gaussian). However, real-world data often has dependencies at multiple scales. For instance, in images, high-level features (like object shapes) influence lower-level features (like textures or pixel details).\n",
    "\n",
    "A **hierarchical prior** is a prior distribution defined over other priors, creating multiple levels of latent variables. This allows the model to capture multi-scale dependencies, where each level's latent variables are conditioned on the ones above it.\n",
    "\n",
    "## 3. How Hierarchical VAEs Work:\n",
    "In a hierarchical VAE, the latent space is organized into multiple layers, with each layer capturing different levels of abstraction in the data:\n",
    "- The **top layer** captures high-level features (e.g., objects, shapes).\n",
    "- The **bottom layer** captures low-level features (e.g., textures, pixel-level details).\n",
    "\n",
    "For example, in a 2-layer hierarchical VAE:\n",
    "- The first layer might capture global object shapes (high-level features).\n",
    "- The second layer would capture local details (like textures or edges), conditioned on the first layer's output.\n",
    "\n",
    "Each layer has its own prior, and the latent variables are sampled using the reparameterization trick. The layers are connected probabilistically, allowing the model to generate more structured and realistic data.\n",
    "\n",
    "## 4. Training Hierarchical VAEs:\n",
    "The training process involves the following steps:\n",
    "1. **Encoding**: The encoder computes the parameters (mean and variance) for the latent variables at each layer and samples from these distributions.\n",
    "2. **Decoding**: The decoder reconstructs the input data using all levels of latent variables.\n",
    "3. **Loss Calculation**:\n",
    "   - **Reconstruction Loss**: Measures the difference between the input data and the reconstructed output.\n",
    "   - **KL Divergence Loss**: Regularizes the latent space, ensuring each layerâ€™s latent distribution matches the prior, with dependencies across layers.\n",
    "4. **Backpropagation**: Gradients are computed for both losses, and the model parameters are updated using an optimization method (e.g., Adam optimizer).\n",
    "5. **Repeat**: The process continues for several epochs, with the model learning better representations of data at multiple scales.\n",
    "\n",
    "## 5. Benefits of Hierarchical Priors:\n",
    "- **Improved Representations**: Hierarchical priors enable the model to learn richer, more structured latent representations that capture multi-level dependencies in data.\n",
    "- **Better Generalization**: By modeling high-level and low-level features separately, hierarchical VAEs can generalize better on complex datasets.\n",
    "- **More Expressive**: They can handle more complex and diverse data distributions (e.g., images, text, videos) by allowing each level of the latent space to represent different types of features.\n",
    "\n",
    "## 6. Applications:\n",
    "- **Image Generation**: Hierarchical VAEs can generate realistic images by capturing both the overall structure (e.g., object shapes) and detailed features (e.g., textures).\n",
    "- **Text Generation**: In natural language, hierarchical VAEs can learn dependencies between high-level topics and low-level words, producing more coherent and contextually rich text.\n",
    "- **Anomaly Detection**: By learning a multi-scale representation, hierarchical VAEs can better detect anomalies since outliers are less likely to fit the complex multi-level structure of normal data.\n",
    "- **Semi-Supervised Learning**: Hierarchical VAEs help learn from both labeled and unlabeled data by leveraging the structured latent space, improving performance when labeled data is scarce.\n",
    "\n",
    "## 7. Challenges:\n",
    "- **Model Complexity**: Hierarchical VAEs are more complex than standard VAEs, requiring careful design and tuning of the latent layers and priors.\n",
    "- **Optimization**: Training hierarchical VAEs can be more difficult due to the increased number of parameters and dependencies between latent variables.\n",
    "- **Computational Cost**: With multiple layers and priors, these models may require more memory and longer training times.\n",
    "\n",
    "## 8. Key Differences from Standard VAEs:\n",
    "- **Prior Structure**: Standard VAEs use a simple Gaussian prior for each latent variable, while hierarchical VAEs define priors across multiple layers of latent variables, capturing complex dependencies.\n",
    "- **Modeling Power**: Hierarchical VAEs can model more complex relationships in the data, whereas standard VAEs assume that the latent variables are independent.\n",
    "\n",
    "## 9. Future Directions:\n",
    "Research on hierarchical VAEs is still evolving. Potential improvements include:\n",
    "- **Handling Temporal Data**: Extending hierarchical VAEs for sequential data (e.g., videos, time series).\n",
    "- **Multimodal Learning**: Combining text, images, and audio for richer generative models.\n",
    "- **Better Optimization**: Developing more efficient training methods for hierarchical structures.\n",
    "\n",
    "### Conclusion:\n",
    "Hierarchical VAEs offer a powerful extension to standard VAEs by introducing multiple levels of latent variables, each capturing different aspects of data. They provide a more flexible and expressive framework for generative modeling, particularly for complex datasets like images and text. By learning multi-scale representations, hierarchical VAEs can generate more realistic data, improve generalization, and handle a wide variety of tasks, from image synthesis to anomaly detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa1398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Encoder\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dense1 = layers.Dense(400, activation='relu')\n",
    "        self.mean = layers.Dense(latent_dim)\n",
    "        self.logvar = layers.Dense(latent_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.mean(x), self.logvar(x)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dense1 = layers.Dense(400, activation='relu')\n",
    "        self.output_layer = layers.Dense(784, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "# Hierarchical VAE Model with Latent Layers\n",
    "class HierarchicalVAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim=2, num_latent_layers=2):\n",
    "        super(HierarchicalVAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.num_latent_layers = num_latent_layers\n",
    "        self.latent_layers = [layers.Dense(latent_dim) for _ in range(num_latent_layers)]\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        epsilon = tf.random.normal(shape=tf.shape(mu))\n",
    "        return mu + tf.exp(0.5 * logvar) * epsilon\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mu, logvar = self.encoder(inputs)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        for latent_layer in self.latent_layers:\n",
    "            z = latent_layer(z)  # Apply hierarchical layers\n",
    "            \n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "\n",
    "# Loss function\n",
    "def compute_loss(recon_x, x, mu, logvar):\n",
    "    cross_entropy = tf.reduce_sum(\n",
    "        tf.keras.losses.binary_crossentropy(x, recon_x), axis=[1, 2]\n",
    "    )\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    kl_divergence = -0.5 * tf.reduce_mean(\n",
    "        tf.reduce_sum(1 + logvar - tf.square(mu) - tf.exp(logvar), axis=1)\n",
    "    )\n",
    "    \n",
    "    return cross_entropy + kl_divergence\n",
    "\n",
    "\n",
    "# Loading MNIST dataset\n",
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_train = x_train.reshape(-1, 784).astype(np.float32)\n",
    "x_test = x_test.reshape(-1, 784).astype(np.float32)\n",
    "\n",
    "# Create a TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(60000).batch(64)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(x_test).batch(64)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = HierarchicalVAE(latent_dim=2, num_latent_layers=3)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for data in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = compute_loss(recon_batch, data, mu, logvar)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        train_loss += loss\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss / len(train_dataset)}')\n",
    "\n",
    "\n",
    "# Visualization of reconstructed images\n",
    "def visualize_reconstruction(model, dataset):\n",
    "    for data in dataset.take(1):\n",
    "        recon_batch, _, _ = model(data)\n",
    "        recon_batch = tf.reshape(recon_batch, (-1, 28, 28))\n",
    "        plt.imshow(recon_batch[0].numpy(), cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "visualize_reconstruction(model, test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
