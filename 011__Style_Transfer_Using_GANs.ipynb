{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "qs7QO6jyzuK4",
        "outputId": "ef7253e9-c1e3-4d8a-e5f0-0b488fb61e5a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "{{function_node __wrapped__DecodeImage_device_/job:localhost/replica:0/task:0/device:CPU:0}} Invalid PNG data, size 1048576 [Op:DecodeImage] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c2f32c2545ac>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mcontent_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_preprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mstyle_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_preprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 2. VGG19 model for extracting features (same as before)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-c2f32c2545ac>\u001b[0m in \u001b[0;36mload_and_preprocess_image\u001b[0;34m(path, target_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_preprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg19\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5982\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5983\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__DecodeImage_device_/job:localhost/replica:0/task:0/device:CPU:0}} Invalid PNG data, size 1048576 [Op:DecodeImage] name: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG19\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to load and preprocess content and style images\n",
        "def load_and_preprocess_image(path, target_size=(224, 224)):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_image(img)\n",
        "    img = tf.image.resize(img, target_size)\n",
        "    img = tf.keras.applications.vgg19.preprocess_input(img)\n",
        "    return tf.expand_dims(img, axis=0)\n",
        "\n",
        "# 1. Load sample data (use Naruto content and style images)\n",
        "content_image_path = '/content/wp4642409-naruto-manga-wallpapers.jpg'  # Replace with your custom content image path\n",
        "style_image_path = '/content/ancient-japan-background-digital-art-style.jpg.crdownload'    # Replace with your custom style image path\n",
        "\n",
        "content_image = load_and_preprocess_image(content_image_path)\n",
        "style_image = load_and_preprocess_image(style_image_path)\n",
        "\n",
        "# 2. VGG19 model for extracting features (same as before)\n",
        "vgg = VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "def get_vgg_model(vgg, content_layer, style_layers):\n",
        "    content_output = vgg.get_layer(content_layer).output\n",
        "    style_outputs = [vgg.get_layer(layer).output for layer in style_layers]\n",
        "    return tf.keras.Model(inputs=vgg.input, outputs=[content_output] + style_outputs)\n",
        "\n",
        "content_layer = 'block5_conv2'\n",
        "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
        "\n",
        "vgg_model = get_vgg_model(vgg, content_layer, style_layers)\n",
        "content_output = vgg_model(content_image)\n",
        "style_output = vgg_model(style_image)\n",
        "\n",
        "# Generate an initial image to optimize\n",
        "generated_image = tf.Variable(content_image, dtype=tf.float32)\n",
        "\n",
        "# Compute loss and train step (same as before)\n",
        "def compute_loss(generated_image, content_output, style_output):\n",
        "    generated_features = vgg_model(generated_image)\n",
        "    content_loss = tf.reduce_mean((generated_features[0] - content_output[0]) ** 2)\n",
        "    style_loss = 0\n",
        "    for i in range(len(style_layers)):\n",
        "        generated_style = generated_features[i + 1]\n",
        "        style = style_output[i + 1]\n",
        "        gram_generated = tf.linalg.einsum('bijc,bijd->bcd', generated_style, generated_style)\n",
        "        gram_style = tf.linalg.einsum('bijc,bijd->bcd', style, style)\n",
        "        gram_generated /= tf.cast(tf.shape(generated_style)[1] * tf.shape(generated_style)[2], tf.float32)\n",
        "        gram_style /= tf.cast(tf.shape(style)[1] * tf.shape(style)[2], tf.float32)\n",
        "        style_loss += tf.reduce_mean((gram_generated - gram_style) ** 2)\n",
        "    total_loss = content_loss + (0.01 * style_loss)\n",
        "    return total_loss\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.02)\n",
        "\n",
        "@tf.function()\n",
        "def train_step(generated_image, content_output, style_output):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = compute_loss(generated_image, content_output, style_output)\n",
        "    gradients = tape.gradient(loss, generated_image)\n",
        "    optimizer.apply_gradients([(gradients, generated_image)])\n",
        "    generated_image.assign(tf.clip_by_value(generated_image, 0.0, 255.0))\n",
        "\n",
        "# Train the model\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    train_step(generated_image, content_output, style_output)\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {compute_loss(generated_image, content_output, style_output)}\")\n",
        "        img = generated_image.numpy()\n",
        "        img = np.squeeze(img, axis=0)\n",
        "        img = img.astype('uint8')\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n"
      ]
    }
  ]
}