{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Handwriting Generation using Conditional GANs (cGANs)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Handwriting generation is a challenging task in generative AI that involves generating images of handwritten text that mimic natural human handwriting. By conditioning a generative model on specific text inputs, we can control the text that is generated by the model. **Conditional GANs (cGANs)** are particularly useful for this task as they allow us to generate data (images) conditioned on a specific label or input—in this case, a text label.\n",
        "\n",
        "### What is a Conditional GAN (cGAN)?\n",
        "\n",
        "A **Conditional Generative Adversarial Network (cGAN)** is a type of GAN where both the generator and discriminator receive additional information besides random noise. In handwriting generation, this additional information is typically the text or the label (e.g., \"hello\", \"world\"). This allows the model to generate handwriting that corresponds to specific textual input.\n",
        "\n",
        "- **Generator**: The generator in a cGAN creates data (images) conditioned on a specific input, such as the text label. The generator takes two inputs:\n",
        "  1. A random noise vector (latent vector).\n",
        "  2. A textual representation (e.g., a one-hot encoded vector or an embedding vector representing the text to be written).\n",
        "  \n",
        "- **Discriminator**: The discriminator tries to distinguish between real and fake images, and is also conditioned on the text. It takes two inputs:\n",
        "  1. A handwritten image (real or generated).\n",
        "  2. The text label (the conditioning information).\n",
        "\n",
        "The generator's goal is to create realistic handwriting images that correspond to the text input, while the discriminator's goal is to correctly identify whether an image is real or generated.\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "### 1. Generator\n",
        "\n",
        "The **generator** is responsible for generating handwriting images based on the given text input. The generator receives:\n",
        "- A random noise vector (latents).\n",
        "- A text vector (the condition) representing the target word/phrase to be written.\n",
        "\n",
        "The generator processes these inputs through dense layers and upsampling layers (e.g., **transposed convolutions**) to generate the image. The text conditioning is typically incorporated by concatenating the text vector to the noise vector, allowing the generator to control the content of the generated handwriting.\n",
        "\n",
        "### 2. Discriminator\n",
        "\n",
        "The **discriminator** evaluates whether an image is real or fake. It receives:\n",
        "- An image (handwritten text).\n",
        "- A text vector representing the condition.\n",
        "\n",
        "The discriminator is a convolutional neural network (CNN) that processes the image and text, comparing the two to determine whether the image corresponds to the correct text. It outputs a probability indicating whether the image is real (from the dataset) or fake (generated).\n",
        "\n",
        "### 3. Loss Functions\n",
        "\n",
        "- **Generator Loss**: The generator is trained to minimize the ability of the discriminator to distinguish between real and fake images. The loss is computed by using binary cross-entropy to measure the difference between the discriminator's prediction for fake images and the target label of 1 (real).\n",
        "\n",
        "- **Discriminator Loss**: The discriminator is trained to correctly classify real and fake images. It tries to maximize the difference between the predictions for real images (target label = 1) and fake images (target label = 0).\n",
        "\n",
        "### 4. Training Process\n",
        "\n",
        "The training process alternates between training the **discriminator** and the **generator**:\n",
        "1. **Discriminator Update**: The discriminator is trained to differentiate between real and generated images, conditioned on the text.\n",
        "2. **Generator Update**: The generator is trained to fool the discriminator into classifying generated images as real, conditioned on the text.\n",
        "\n",
        "The generator and discriminator are trained iteratively, improving their performance over time. The generator learns to generate more realistic handwriting, and the discriminator becomes better at distinguishing real from fake images.\n",
        "\n",
        "## Benefits of Conditional GANs for Handwriting Generation\n",
        "\n",
        "- **Control over Generated Content**: By conditioning the generator on the text, we have fine-grained control over the content of the generated handwriting. This allows us to generate any desired word or sentence.\n",
        "  \n",
        "- **Realistic Handwriting Generation**: cGANs are capable of generating realistic and coherent handwriting that resembles human-written text.\n",
        "\n",
        "- **Flexibility**: By using embeddings or other forms of text representation, cGANs can work with complex text inputs (e.g., multi-word sentences, various handwriting styles).\n",
        "\n",
        "## Key Challenges\n",
        "\n",
        "- **Complexity of Handwriting Styles**: Handwriting generation involves dealing with variability in handwriting styles, sizes, and strokes. Capturing this complexity requires large, diverse datasets and advanced model architectures.\n",
        "\n",
        "- **Training Stability**: GANs, in general, are known for their instability during training. Proper tuning of the hyperparameters, including the learning rates for both the generator and discriminator, is essential for achieving stable training.\n",
        "\n",
        "## Applications of Handwriting Generation\n",
        "\n",
        "- **Digital Document Creation**: Automatically generate handwritten text for various digital applications (e.g., emails, notes).\n",
        "  \n",
        "- **Personalization**: Create personalized handwritten messages based on specific user input or preferences.\n",
        "\n",
        "- **Handwriting Synthesis**: Generate handwritten content that mimics a particular person's handwriting, useful for applications like digital signatures or stylized writing.\n",
        "\n",
        "- **OCR and Text Recognition**: Handwriting generation models can also be used to augment OCR (Optical Character Recognition) training datasets by generating synthetic handwritten data for specific words or phrases.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Handwriting generation using Conditional GANs (cGANs) is a powerful approach for generating realistic handwritten text based on specific input conditions. By leveraging both random noise and text embeddings, cGANs allow for controlled generation of handwriting, which can be used in various applications such as digital document creation, personalized messages, and handwriting synthesis. The combination of adversarial training and text conditioning enables the generation of high-quality, coherent handwriting images that are aligned with the given textual input.\n",
        "\n"
      ],
      "metadata": {
        "id": "L1PtSoEKE0Xk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "waQXnr5kDRYf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    # Assuming you load your custom handwriting dataset where images and corresponding texts are available\n",
        "    # For demonstration purposes, using MNIST as a placeholder.\n",
        "    (x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Normalize images to [-1, 1] range\n",
        "    x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
        "    x_train = np.expand_dims(x_train, axis=-1)  # Add channel dimension\n",
        "\n",
        "    # Encode text labels (for conditional generation) - e.g., use text embedding or one-hot encoding\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 10)  # Adjust this for your dataset\n",
        "\n",
        "    return x_train, y_train\n"
      ],
      "metadata": {
        "id": "AMoGOym7DvOt"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_generator(latent_dim, text_dim):\n",
        "    # Separate inputs for noise and text\n",
        "    noise_input = layers.Input(shape=(latent_dim,))\n",
        "    text_input = layers.Input(shape=(text_dim,))\n",
        "\n",
        "    # Combine noise vector and text embedding\n",
        "    merged_input = layers.concatenate([noise_input, text_input])  # Concatenate noise + text\n",
        "    x = layers.Dense(128 * 7 * 7, activation='relu')(merged_input)  # Adjusted dense layer output\n",
        "    x = layers.Reshape((7, 7, 128))(x)  # Reshape to a smaller starting size\n",
        "\n",
        "    # Transposed convolution layers to generate the image\n",
        "    # Adjusted strides and kernel sizes to reach 28x28 output\n",
        "    x = layers.Conv2DTranspose(64, kernel_size=3, strides=2, padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding='same', activation='tanh')(x)\n",
        "    # Removed an extra Conv2DTranspose layer and adjusted the last one\n",
        "\n",
        "    # Create the model with two inputs and one output\n",
        "    model = models.Model(inputs=[noise_input, text_input], outputs=x)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "gpdJrKB_DvLE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_discriminator(image_shape, text_dim):\n",
        "    image_input = layers.Input(shape=image_shape)\n",
        "    text_input = layers.Input(shape=(text_dim,))\n",
        "\n",
        "    # Convolutional layers to process image\n",
        "    x = layers.Conv2D(64, kernel_size=5, strides=2, padding='same', activation='relu')(image_input)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=5, strides=2, padding='same', activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # Flatten and combine with text embedding\n",
        "    x = layers.Flatten()(x)\n",
        "    text_embedding = layers.Dense(128, activation='relu')(text_input)\n",
        "    x = layers.concatenate([x, text_embedding])  # Concatenate image features and text features\n",
        "\n",
        "    # Final dense layer for classification (real or fake)\n",
        "    x = layers.Dense(1)(x)\n",
        "\n",
        "    model = models.Model(inputs=[image_input, text_input], outputs=x)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "3JSd7Q4UDvId"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    return real_loss + fake_loss\n"
      ],
      "metadata": {
        "id": "-4YcCYDpDvF1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(images, text, generator, discriminator, gen_optimizer, disc_optimizer, latent_dim):\n",
        "    batch_size = tf.shape(images)[0]  # Get batch size from images\n",
        "\n",
        "    noise = tf.random.normal([BATCH_SIZE, latent_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator([noise, text], training=True)\n",
        "\n",
        "        real_output = discriminator([images, text], training=True)\n",
        "        fake_output = discriminator([generated_images, text], training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "def train(dataset, epochs, generator, discriminator, gen_optimizer, disc_optimizer, latent_dim):\n",
        "    for epoch in range(epochs):\n",
        "        for image_batch, text_batch in dataset:\n",
        "            train_step(image_batch, text_batch, generator, discriminator, gen_optimizer, disc_optimizer, latent_dim)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n"
      ],
      "metadata": {
        "id": "DlVD-l0xDvDP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 100  # Noise vector size\n",
        "text_dim = 10  # Example for one-hot encoding of digits (adjust for your dataset)\n",
        "\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "\n",
        "# Load data\n",
        "x_train, y_train = load_data()\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(BATCH_SIZE)\n",
        "\n",
        "# Build models\n",
        "generator = build_generator(latent_dim, text_dim)\n",
        "discriminator = build_discriminator(image_shape=(28, 28, 1), text_dim=text_dim)\n",
        "\n",
        "# Optimizers\n",
        "gen_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "disc_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "# Train the model\n",
        "train(train_dataset, EPOCHS, generator, discriminator, gen_optimizer, disc_optimizer, latent_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "XqRoTozuDvAb",
        "outputId": "efc9d328-e08c-487d-8421-12d927da722c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"<ipython-input-24-f5d4bc241b67>\", line 8, in train_step  *\n        generated_images = generator([noise, text], training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling Concatenate.call().\n    \n    \u001b[1mDimension 0 in both shapes must be equal, but are 64 and 32. Shapes are [64] and [32]. for '{{node functional_16_1/concatenate_9_1/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](random_normal, functional_16_1/Cast, functional_16_1/concatenate_9_1/concat/axis)' with input shapes: [64,100], [32,10], [] and with computed input tensors: input[2] = <-1>.\u001b[0m\n    \n    Arguments received by Concatenate.call():\n      • inputs=['tf.Tensor(shape=(64, 100), dtype=float32)', 'tf.Tensor(shape=(32, 10), dtype=float32)']\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-7e9bb143d309>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-6b7a63ea9a17>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs, generator, discriminator, gen_optimizer, disc_optimizer, latent_dim)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_file17_embdk.py\u001b[0m in \u001b[0;36mtf__train_step\u001b[0;34m(images, text, generator, discriminator, gen_optimizer, disc_optimizer, latent_dim)\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                     \u001b[0mgenerated_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                     \u001b[0mreal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mfake_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-24-f5d4bc241b67>\", line 8, in train_step  *\n        generated_images = generator([noise, text], training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling Concatenate.call().\n    \n    \u001b[1mDimension 0 in both shapes must be equal, but are 64 and 32. Shapes are [64] and [32]. for '{{node functional_16_1/concatenate_9_1/concat}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](random_normal, functional_16_1/Cast, functional_16_1/concatenate_9_1/concat/axis)' with input shapes: [64,100], [32,10], [] and with computed input tensors: input[2] = <-1>.\u001b[0m\n    \n    Arguments received by Concatenate.call():\n      • inputs=['tf.Tensor(shape=(64, 100), dtype=float32)', 'tf.Tensor(shape=(32, 10), dtype=float32)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_generated_images(generator, epoch, latent_dim, text_dim):\n",
        "    noise = tf.random.normal([16, latent_dim])\n",
        "    generated_images = generator([noise, np.ones((16, text_dim))], training=False)  # Example for text label = \"1\"\n",
        "    generated_images = (generated_images + 1) / 2.0  # Rescale to [0, 1]\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    for i in range(16):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        plt.imshow(generated_images[i, :, :, 0], cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(f\"Epoch {epoch}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "uAyy5AwvDu9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yLlHFrzEFtwl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}