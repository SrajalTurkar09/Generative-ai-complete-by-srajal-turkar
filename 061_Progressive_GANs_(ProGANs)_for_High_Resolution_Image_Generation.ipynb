{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup and Libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "p8cNVxkcgLBo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Ipsl5Xrdffga"
      },
      "outputs": [],
      "source": [
        "# Import TensorFlow and required modules\n",
        "import tensorflow as tf\n",
        "\n",
        "# Import Keras layers and models for building the ProGAN\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "# Import NumPy for data handling\n",
        "import numpy as np\n",
        "\n",
        "# Import Matplotlib for visualizing generated images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import OS for saving/loading models or generated images\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Key Components of ProGAN\n"
      ],
      "metadata": {
        "id": "Ml_FYKeqgPqV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a. The Generator**"
      ],
      "metadata": {
        "id": "fNfBUtFogUNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the generator model\n",
        "def build_generator(latent_dim, channels):\n",
        "    \"\"\"\n",
        "    Build the initial generator model for 4x4 resolution.\n",
        "    latent_dim: Size of the input noise vector\n",
        "    channels: Number of filters in the initial convolutional layer\n",
        "    \"\"\"\n",
        "    # Define the input layer for the latent vector (noise)\n",
        "    inputs = layers.Input(shape=(latent_dim,))\n",
        "\n",
        "    # First dense layer to transform the noise into a 4x4xChannels feature map\n",
        "    x = layers.Dense(4 * 4 * channels)(inputs)\n",
        "\n",
        "    # Reshape the dense layer output into 4x4 spatial dimensions\n",
        "    x = layers.Reshape((4, 4, channels))(x)\n",
        "\n",
        "    # Apply LeakyReLU activation for non-linearity\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # Build and return the initial generator model\n",
        "    model = models.Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Define a function to progressively add blocks to the generator\n",
        "def add_generator_block(generator, filters):\n",
        "    \"\"\"\n",
        "    Add a higher-resolution block to the generator.\n",
        "    generator: Existing generator model\n",
        "    filters: Number of filters for the new convolutional layers\n",
        "    \"\"\"\n",
        "    # Retain the original input of the generator\n",
        "    inputs = generator.input\n",
        "\n",
        "    # Get the current output of the generator\n",
        "    x = generator.output\n",
        "\n",
        "    # Upsample the current resolution (e.g., from 4x4 to 8x8)\n",
        "    x = layers.UpSampling2D()(x)\n",
        "\n",
        "    # Add a convolutional layer to learn finer details\n",
        "    x = layers.Conv2D(filters, kernel_size=3, padding=\"same\")(x)\n",
        "\n",
        "    # Apply LeakyReLU activation for non-linearity\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # Add a final convolutional layer to map to RGB output (3 channels)\n",
        "    rgb_output = layers.Conv2D(3, kernel_size=1, padding=\"same\", activation=\"tanh\")(x)\n",
        "\n",
        "    # Build the updated generator model with the new block\n",
        "    model = models.Model(inputs, rgb_output)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "B1Mi10Fvfljg"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. The Discriminator**"
      ],
      "metadata": {
        "id": "n049aPA8gZLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the discriminator model\n",
        "def build_discriminator(channels):\n",
        "    \"\"\"\n",
        "    Build the initial discriminator model for 4x4 resolution.\n",
        "    channels: Number of filters in the initial convolutional layer\n",
        "    \"\"\"\n",
        "    # Define the input layer for images of resolution 4x4 with 3 channels (RGB)\n",
        "    inputs = layers.Input(shape=(4, 4, 3))\n",
        "\n",
        "    # First convolutional layer to extract features\n",
        "    x = layers.Conv2D(channels, kernel_size=3, padding=\"same\")(inputs)\n",
        "\n",
        "    # Apply LeakyReLU activation for non-linearity\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    # Flatten the feature map for the dense output layer\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    # Dense layer to predict real (1) or fake (0)\n",
        "    x = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    # Build and return the initial discriminator model\n",
        "    model = models.Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Define a function to progressively add blocks to the discriminator\n",
        "def add_discriminator_block(discriminator, filters):\n",
        "    \"\"\"\n",
        "    Add a higher-resolution block to the discriminator.\n",
        "    discriminator: Existing discriminator model\n",
        "    filters: Number of filters for the new convolutional layers\n",
        "    \"\"\"\n",
        "    # Define a new input layer for higher-resolution images\n",
        "    inputs = layers.Input(shape=(None, None, 3))  # Input accepts variable resolution\n",
        "\n",
        "    # Downsample the input from higher resolution to lower resolution\n",
        "    x = layers.AveragePooling2D(pool_size=(2, 2))(inputs)\n",
        "\n",
        "    # Pass the downsampled input to the existing discriminator\n",
        "    x = discriminator(x)\n",
        "\n",
        "    # Build and return the updated discriminator model\n",
        "    model = models.Model(inputs, x)\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "oA2Pt8UsfniT"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Training Process\n"
      ],
      "metadata": {
        "id": "6QlJja9qgi5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```a. Progressive Growth```\n"
      ],
      "metadata": {
        "id": "EMcm9t34gkt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the progressive growth of ProGAN\n",
        "def grow_progan(generator, discriminator, latent_dim, resolutions, filters, epochs_per_stage):\n",
        "    \"\"\"\n",
        "    Gradually grow the ProGAN by adding layers to the generator and discriminator.\n",
        "    generator: Generator model\n",
        "    discriminator: Discriminator model\n",
        "    latent_dim: Size of the input noise vector\n",
        "    resolutions: List of target resolutions (e.g., [4, 8, 16, 32])\n",
        "    filters: List of filters for each resolution\n",
        "    epochs_per_stage: Number of epochs to train at each resolution\n",
        "    \"\"\"\n",
        "    # Iterate through the target resolutions and filters\n",
        "    for resolution, filter_count in zip(resolutions, filters):\n",
        "        # Update the generator with a new block\n",
        "        generator = add_generator_block(generator, filter_count)\n",
        "\n",
        "        # Update the discriminator with a new block\n",
        "        discriminator = add_discriminator_block(discriminator, filter_count)\n",
        "\n",
        "        # Prepare to train models at the current resolution\n",
        "        print(f\"Training at resolution: {resolution}x{resolution}\")\n",
        "        train(generator, discriminator, latent_dim, resolution, epochs_per_stage)\n"
      ],
      "metadata": {
        "id": "x_fGxo34frlo"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```b. Loss Functions and Optimizers```"
      ],
      "metadata": {
        "id": "5a6fgevOgqmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the generator loss function\n",
        "def generator_loss(fake_output):\n",
        "    \"\"\"\n",
        "    Calculates the loss for the generator.\n",
        "    fake_output: Discriminator predictions for fake images\n",
        "    \"\"\"\n",
        "    return tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "\n",
        "# Define the discriminator loss function\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    \"\"\"\n",
        "    Calculates the loss for the discriminator.\n",
        "    real_output: Discriminator predictions for real images\n",
        "    fake_output: Discriminator predictions for fake images\n",
        "    \"\"\"\n",
        "    # Loss for real images (should output 1)\n",
        "    real_loss = tf.keras.losses.binary_crossentropy(tf.ones_like(real_output), real_output)\n",
        "\n",
        "    # Loss for fake images (should output 0)\n",
        "    fake_loss = tf.keras.losses.binary_crossentropy(tf.zeros_like(fake_output), fake_output)\n",
        "\n",
        "    # Return the total discriminator loss\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "\n",
        "# Define Adam optimizers for both generator and discriminator\n",
        "generator_optimizer = optimizers.Adam(learning_rate=0.001, beta_1=0.0, beta_2=0.99)\n",
        "discriminator_optimizer = optimizers.Adam(learning_rate=0.001, beta_1=0.0, beta_2=0.99)\n"
      ],
      "metadata": {
        "id": "E7jm14csftfU"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```c. Training Loop```"
      ],
      "metadata": {
        "id": "uGn_Ulbrg4Te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a single training step for ProGAN\n",
        "@tf.function\n",
        "def train_step(generator, discriminator, images, latent_dim, resolution):\n",
        "    \"\"\"\n",
        "    Executes one step of training for the generator and discriminator.\n",
        "    generator: Generator model\n",
        "    discriminator: Discriminator model\n",
        "    images: Real images from the dataset\n",
        "    latent_dim: Size of the input noise vector\n",
        "    resolution: Current resolution for training\n",
        "    \"\"\"\n",
        "    # Resize real images to the current resolution\n",
        "    images = tf.image.resize(images, (resolution, resolution))\n",
        "\n",
        "    # Generate random noise vectors for the generator\n",
        "    noise = tf.random.normal([images.shape[0], latent_dim])\n",
        "\n",
        "    # Use GradientTape to calculate gradients for both models\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        # Generate fake images using the generator\n",
        "        fake_images = generator(noise, training=True)\n",
        "\n",
        "        # Discriminator predictions for real and fake images\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(fake_images, training=True)\n",
        "\n",
        "        # Compute the generator and discriminator losses\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    # Calculate gradients for the generator and discriminator\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    # Apply gradients to update the generator's weights\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n",
        "    # Apply gradients to update the discriminator's weights\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    # Return the generator and discriminator losses for logging\n",
        "    return gen_loss, disc_loss\n"
      ],
      "metadata": {
        "id": "t8ZExQgifu0n"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```Training Function```\n"
      ],
      "metadata": {
        "id": "F5m4-26Wg9xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training loop for ProGAN\n",
        "def train(generator, discriminator, latent_dim, resolution, epochs):\n",
        "    \"\"\"\n",
        "    Trains the generator and discriminator at the given resolution.\n",
        "    generator: Generator model\n",
        "    discriminator: Discriminator model\n",
        "    latent_dim: Size of the input noise vector\n",
        "    resolution: Target resolution for the current training stage\n",
        "    epochs: Number of epochs to train at the current resolution\n",
        "    \"\"\"\n",
        "    # Load CIFAR-10 dataset\n",
        "    (x_train, _), _ = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "    # Normalize pixel values to [-1, 1]\n",
        "    x_train = (x_train.astype(\"float32\") - 127.5) / 127.5\n",
        "\n",
        "    # Resize images to the target resolution\n",
        "    x_train = tf.image.resize(x_train, [resolution, resolution]) # Resize images to the target resolution\n",
        "\n",
        "    # Convert to a TensorFlow dataset for efficient batching\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(64)\n",
        "\n",
        "    # Train for the specified number of epochs\n",
        "    for epoch in range(epochs):\n",
        "        for batch in dataset:  # Loop through each batch in the dataset\n",
        "            # Perform a single training step at the current resolution\n",
        "            g_loss, d_loss = train_step(generator, discriminator, batch, latent_dim, resolution)\n",
        "\n",
        "        # Print loss metrics after each epoch\n",
        "        print(f\"Epoch {epoch + 1}, Generator Loss: {g_loss.numpy()}, Discriminator Loss: {d_loss.numpy()}\")"
      ],
      "metadata": {
        "id": "b1Noa_b7f7LV"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Generate Samples"
      ],
      "metadata": {
        "id": "1ChkWNlVhDoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate and visualize samples\n",
        "def generate_samples(generator, latent_dim, num_samples=16):\n",
        "    \"\"\"\n",
        "    Generates and displays a grid of images using the trained generator.\n",
        "    generator: Trained generator model\n",
        "    latent_dim: Size of the input noise vector\n",
        "    num_samples: Number of images to generate\n",
        "    \"\"\"\n",
        "    # Generate random noise vectors\n",
        "    noise = tf.random.normal([num_samples, latent_dim])\n",
        "\n",
        "    # Generate fake images using the generator\n",
        "    samples = generator(noise, training=False)\n",
        "\n",
        "    # Denormalize the pixel values to the range [0, 1]\n",
        "    samples = (samples + 1) / 2\n",
        "\n",
        "    # Plot the images in a grid\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(4, 4, i + 1)  # Create a 4x4 grid\n",
        "        plt.imshow(samples[i])\n",
        "        plt.axis(\"off\")  # Hide axes\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "UOFCkU7Uf-Ev"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Putting It All Together\n",
        "\n"
      ],
      "metadata": {
        "id": "eqomqgTNhGzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the latent dimension for the noise vector\n",
        "latent_dim = 128\n",
        "\n",
        "# Define the number of filters for the initial layers\n",
        "initial_channels = 64\n",
        "\n",
        "# Define the progressive resolutions and corresponding filters\n",
        "resolutions = [4, 8, 16, 32, 64]\n",
        "filters = [64, 128, 256, 512, 512]  # Filters for each resolution\n",
        "\n",
        "# Define the number of epochs per stage of resolution\n",
        "epochs_per_stage = 5\n",
        "\n",
        "# Build the initial generator and discriminator for 4x4 resolution\n",
        "generator = build_generator(latent_dim, initial_channels)\n",
        "discriminator = build_discriminator(initial_channels)\n",
        "\n",
        "# Train the ProGAN by progressively growing the generator and discriminator\n",
        "grow_progan(generator, discriminator, latent_dim, resolutions, filters, epochs_per_stage)\n",
        "\n",
        "# Save the trained generator and discriminator models\n",
        "generator.save(\"progan_generator.h5\")\n",
        "discriminator.save(\"progan_discriminator.h5\")\n",
        "\n",
        "# Generate and visualize samples from the trained generator\n",
        "generate_samples(generator, latent_dim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "QYys5xHggAlh",
        "outputId": "e60cdf05-2a68-4244-d040-31ab2cc33162"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training at resolution: 4x4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"<ipython-input-35-8bd595d9e467>\", line 24, in train_step  *\n        real_output = discriminator(images, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\", line 245, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling Functional.call().\n    \n    \u001b[1mInput 0 of layer \"functional_18\" is incompatible with the layer: expected shape=(None, 4, 4, 3), found shape=(64, 2, 2, 3)\u001b[0m\n    \n    Arguments received by Functional.call():\n      • inputs=tf.Tensor(shape=(64, 4, 4, 3), dtype=float32)\n      • training=True\n      • mask=None\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-c60eb25c325d>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Train the ProGAN by progressively growing the generator and discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mgrow_progan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolutions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_per_stage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Save the trained generator and discriminator models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-4498b22b806e>\u001b[0m in \u001b[0;36mgrow_progan\u001b[0;34m(generator, discriminator, latent_dim, resolutions, filters, epochs_per_stage)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Prepare to train models at the current resolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training at resolution: {resolution}x{resolution}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_per_stage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-70bd0c63c773>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(generator, discriminator, latent_dim, resolution, epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Loop through each batch in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m# Perform a single training step at the current resolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Print loss metrics after each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filesmdx422v.py\u001b[0m in \u001b[0;36mtf__train_step\u001b[0;34m(generator, discriminator, images, latent_dim, resolution)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0mfake_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                     \u001b[0mreal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                     \u001b[0mfake_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0mgen_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m    246\u001b[0m                             \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                             \u001b[0;34m\"incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"<ipython-input-35-8bd595d9e467>\", line 24, in train_step  *\n        real_output = discriminator(images, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\", line 245, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling Functional.call().\n    \n    \u001b[1mInput 0 of layer \"functional_18\" is incompatible with the layer: expected shape=(None, 4, 4, 3), found shape=(64, 2, 2, 3)\u001b[0m\n    \n    Arguments received by Functional.call():\n      • inputs=tf.Tensor(shape=(64, 4, 4, 3), dtype=float32)\n      • training=True\n      • mask=None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u-F5zoAbgB6t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}