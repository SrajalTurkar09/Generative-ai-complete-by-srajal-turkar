{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a4b2bb",
   "metadata": {},
   "source": [
    "# RaLSGAN (Relativistic Average Least Squares GAN) Code Guide\n",
    "This notebook implements **RaLSGAN** using **TensorFlow/Keras** on the MNIST dataset. The code is modular and serves as a reference for future projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f2ef01",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6151c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install TensorFlow if not already installed\n",
    "# !pip install tensorflow\n",
    "\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b699b6b",
   "metadata": {},
   "source": [
    "## Step 2: Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "326bd6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize images to the range [-1, 1]\n",
    "train_images = train_images.reshape(-1, 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5\\\n",
    "\n",
    "\n",
    "\n",
    "# Create a TensorFlow dataset\n",
    "batch_size = 64\n",
    "dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(60000).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9c8046a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_gradients' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerator Gradients:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mgen_gradients\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiscriminator Gradients:\u001b[39m\u001b[38;5;124m\"\u001b[39m, disc_gradients)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gen_gradients' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Generator Gradients:\", gen_gradients)\n",
    "print(\"Discriminator Gradients:\", disc_gradients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9011405",
   "metadata": {},
   "source": [
    "## Step 3: Define the Generator and Discriminator Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6deecc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generator\n",
    "def build_generator(z_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(256, input_dim=z_dim),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(512),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(1024),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(28 * 28, activation='tanh'),\n",
    "        layers.Reshape((28, 28, 1))\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Flatten(input_shape=(28, 28, 1)),\n",
    "        layers.Dense(1024),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(512),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(256),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Dense(1)  # Single scalar output\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc193db2",
   "metadata": {},
   "source": [
    "## Step 4: Define Relativistic Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a3de814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Relativistic Discriminator Loss\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.reduce_mean(tf.square(real_output - tf.reduce_mean(fake_output) - 1))\n",
    "    fake_loss = tf.reduce_mean(tf.square(fake_output - tf.reduce_mean(real_output) + 1))\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "# Relativistic Generator Loss\n",
    "def generator_loss(real_output, fake_output):\n",
    "    real_loss = tf.reduce_mean(tf.square(real_output - tf.reduce_mean(fake_output) + 1))\n",
    "    fake_loss = tf.reduce_mean(tf.square(fake_output - tf.reduce_mean(real_output) - 1))\n",
    "    return real_loss + fake_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a7ad2",
   "metadata": {},
   "source": [
    "## Step 5: Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a8e56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "c:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "z_dim = 100  # Latent space dimension\n",
    "epochs = 50\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# Instantiate Generator and Discriminator\n",
    "generator = build_generator(z_dim)\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Optimizers\n",
    "gen_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04cab9",
   "metadata": {},
   "source": [
    "## Step 6: Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18c69edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_images):\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "    \n",
    "    # Generate random noise and fake images\n",
    "    random_noise = tf.random.normal([batch_size, z_dim])\n",
    "    fake_images = generator(random_noise)\n",
    "    \n",
    "    with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape:\n",
    "        # Compute Discriminator outputs\n",
    "        real_output = discriminator(real_images)\n",
    "        fake_output = discriminator(fake_images)\n",
    "        \n",
    "        # Compute losses\n",
    "        d_loss = discriminator_loss(real_output, fake_output)\n",
    "        g_loss = generator_loss(real_output, fake_output)\n",
    "    \n",
    "    # Compute gradients\n",
    "    disc_gradients = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    gen_gradients = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    \n",
    "    # Debug: Check if gradients are None\n",
    "    print(\"Discriminator Gradients:\", disc_gradients)\n",
    "    print(\"Generator Gradients:\", gen_gradients)\n",
    "    \n",
    "    # Update weights\n",
    "    disc_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n",
    "    gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
    "    \n",
    "    return d_loss, g_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa2904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ee90da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @tf.function\n",
    "# def train_step(real_images):\n",
    "#     batch_size = tf.shape(real_images)[0]\n",
    "    \n",
    "#     # Generate random noise and fake images\n",
    "#     random_noise = tf.random.normal([batch_size, z_dim])\n",
    "#     fake_images = generator(random_noise)\n",
    "    \n",
    "#     with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape:\n",
    "#         # Compute Discriminator outputs\n",
    "#         real_output = discriminator(real_images)\n",
    "#         fake_output = discriminator(fake_images)\n",
    "        \n",
    "#         # Compute losses\n",
    "#         d_loss = discriminator_loss(real_output, fake_output)\n",
    "#         g_loss = generator_loss(real_output, fake_output)\n",
    "    \n",
    "#     # Compute gradients and update weights\n",
    "#     disc_gradients = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "#     gen_gradients = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    \n",
    "#     disc_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n",
    "#     gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
    "    \n",
    "#     return d_loss, g_loss\n",
    "\n",
    "# # Full training loop\n",
    "# def train(dataset, epochs):\n",
    "#     for epoch in range(epochs):\n",
    "#         for batch in dataset:\n",
    "#             d_loss, g_loss = train_step(batch)\n",
    "        \n",
    "#         # Print losses after each epoch\n",
    "#         print(f\"Epoch {epoch + 1}/{epochs}, D Loss: {d_loss.numpy()}, G Loss: {g_loss.numpy()}\")\n",
    "        \n",
    "#         # Generate and visualize images after every epoch\n",
    "#         if (epoch + 1) % 10 == 0:\n",
    "#             generate_and_save_images(generator, epoch + 1, z_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6383c04e",
   "metadata": {},
   "source": [
    "## Step 7: Generate and Save Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "333992b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Gradients: [<tf.Tensor 'AddN_2:0' shape=(784, 1024) dtype=float32>, <tf.Tensor 'AddN_3:0' shape=(1024,) dtype=float32>, <tf.Tensor 'AddN_4:0' shape=(1024, 512) dtype=float32>, <tf.Tensor 'AddN_5:0' shape=(512,) dtype=float32>, <tf.Tensor 'AddN_6:0' shape=(512, 256) dtype=float32>, <tf.Tensor 'AddN_7:0' shape=(256,) dtype=float32>, <tf.Tensor 'AddN_8:0' shape=(256, 1) dtype=float32>, <tf.Tensor 'AddN_9:0' shape=(1,) dtype=float32>]\n",
      "Generator Gradients: [None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Rishu\\AppData\\Local\\Temp\\ipykernel_19068\\1914475880.py\", line 28, in train_step  *\n        gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n    File \"c:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 344, in apply_gradients  **\n        self.apply(grads, trainable_variables)\n    File \"c:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 397, in apply\n        grads, trainable_variables = self._filter_empty_gradients(\n    File \"c:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 729, in _filter_empty_gradients\n        raise ValueError(\"No gradients provided for any variable.\")\n\n    ValueError: No gradients provided for any variable.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataset, epochs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m---> 31\u001b[0m         d_loss, g_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Print losses after each epoch\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, D Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, G Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file2xq_4on1.py:23\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(real_images)\u001b[0m\n\u001b[0;32m     21\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerator Gradients:\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(gen_gradients))\n\u001b[0;32m     22\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(disc_optimizer)\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(disc_gradients), ag__\u001b[38;5;241m.\u001b[39mld(discriminator)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 23\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(gen_optimizer)\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(gen_gradients), ag__\u001b[38;5;241m.\u001b[39mld(generator)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:344\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[0;32m    343\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterations\n",
      "File \u001b[1;32mc:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:397\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[1;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[0;32m    390\u001b[0m grads, trainable_variables \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overwrite_variables_directly_with_gradients(\n\u001b[0;32m    392\u001b[0m         grads, trainable_variables\n\u001b[0;32m    393\u001b[0m     )\n\u001b[0;32m    394\u001b[0m )\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# Filter empty gradients.\u001b[39;00m\n\u001b[1;32m--> 397\u001b[0m grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_empty_gradients(\n\u001b[0;32m    398\u001b[0m     grads, trainable_variables\n\u001b[0;32m    399\u001b[0m )\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(grads)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:729\u001b[0m, in \u001b[0;36mBaseOptimizer._filter_empty_gradients\u001b[1;34m(self, grads, vars)\u001b[0m\n\u001b[0;32m    726\u001b[0m         missing_grad_vars\u001b[38;5;241m.\u001b[39mappend(v\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered_grads:\n\u001b[1;32m--> 729\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_grad_vars:\n\u001b[0;32m    731\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    732\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradients do not exist for variables \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    733\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(missing_grad_vars))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when minimizing the loss.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    734\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m If using `model.compile()`, did you forget to provide a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    735\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`loss` argument?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    736\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Rishu\\AppData\\Local\\Temp\\ipykernel_19068\\1914475880.py\", line 28, in train_step  *\n        gen_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n    File \"c:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 344, in apply_gradients  **\n        self.apply(grads, trainable_variables)\n    File \"c:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 397, in apply\n        grads, trainable_variables = self._filter_empty_gradients(\n    File \"c:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py\", line 729, in _filter_empty_gradients\n        raise ValueError(\"No gradients provided for any variable.\")\n\n    ValueError: No gradients provided for any variable.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_and_save_images(model, epoch, z_dim, num_images=16):\n",
    "    random_noise = tf.random.normal([num_images, z_dim])\n",
    "    fake_images = model(random_noise).numpy()\n",
    "    fake_images = (fake_images + 1) / 2.0  # Rescale to [0, 1]\n",
    "    \n",
    "    # Plot the generated images\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, 2))\n",
    "    for i in range(num_images):\n",
    "        axes[i].imshow(fake_images[i, :, :, 0], cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "    plt.savefig(f\"epoch_{epoch}.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Start training\n",
    "train(dataset, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ddab73a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'read_data_dogimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m truncnorm\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mread_data_dogimage\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'read_data_dogimage'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "import read_data_dogimage\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Conv2D, Reshape, Flatten, concatenate, UpSampling2D, Conv2DTranspose, BatchNormalization, LeakyReLU, GlobalAveragePooling2D, Activation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from PIL import Image\n",
    "\n",
    "if 'tensorflow' == K.backend():\n",
    "    import tensorflow as tf\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "batch_size = 32\n",
    "lr = 0.0005\n",
    "beta1 = 0.5\n",
    "epochs = 30\n",
    "\n",
    "nz = 256\n",
    "\n",
    "\n",
    "def convtlayer(input, filter, kernel_size, stride, padding):\n",
    "    x = Conv2DTranspose(filters=filter, kernel_size=kernel_size, strides=stride, padding=padding, use_bias=False, kernel_initializer='glorot_uniform')(input)\n",
    "    x = BatchNormalization(momentum=0.9, epsilon=1e-05)(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def generator():\n",
    "    input = Input(shape=(nz, ))\n",
    "    x = Reshape((1, 1, nz))(input)\n",
    "    x = convtlayer(x, 1024, 4, 1, 'valid')  # as FC layer\n",
    "    x = convtlayer(x, 512, 4, 2, 'same')\n",
    "    x = convtlayer(x, 256, 4, 2, 'same')\n",
    "    x = convtlayer(x, 128, 4, 2, 'same')\n",
    "    x = convtlayer(x, 64, 4, 2, 'same')\n",
    "    x = Conv2DTranspose(filters=3, kernel_size=3, strides=1, padding='same', use_bias=False, kernel_initializer='glorot_uniform')(x)\n",
    "    x = Activation(activation='tanh')(x)\n",
    "\n",
    "    gene = Model(input, x, name=\"generator\")\n",
    "    gene.summary()\n",
    "\n",
    "    return gene\n",
    "\n",
    "\n",
    "def convlayer(input, filter, kernel_size, stride, padding, bn=False):\n",
    "    x = Conv2D(filters=filter, kernel_size=kernel_size, strides=stride, padding=padding, use_bias=False, kernel_initializer='glorot_uniform')(input)\n",
    "    if bn:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def discriminator():\n",
    "    input = Input(shape=(64, 64, 3))\n",
    "    x = convlayer(input, 32, 4, 2, 'same')\n",
    "    x = convlayer(x, 64, 4, 2, 'same')\n",
    "    x = convlayer(x, 128, 4, 2, 'same', True)\n",
    "    x = convlayer(x, 256, 4, 2, 'same', True)\n",
    "    x = Conv2D(filters=1, kernel_size=4, strides=1, padding='valid', use_bias=False)(x)  # as FC layer\n",
    "\n",
    "    disc = Model(input, x, name=\"discriminator\")\n",
    "    disc.summary()\n",
    "\n",
    "    return disc\n",
    "\n",
    "\n",
    "def train():\n",
    "    gene = generator()\n",
    "    disc = discriminator()\n",
    "\n",
    "    real_img = Input(shape=(64, 64, 3))\n",
    "    noise_input = Input(shape=(nz, ))\n",
    "\n",
    "    # Build Generator Network\n",
    "    fake_img = gene(noise_input)\n",
    "    # Build 2 Discriminator Networks (one from noise input, one from generated samples)\n",
    "    disc_real = disc(real_img)  # C(x_r)\n",
    "    disc_fake = disc(fake_img)  # C(x_f)\n",
    "\n",
    "    # Build Loss\n",
    "    disc_real_average = K.mean(disc_real, axis=0)\n",
    "    disc_fake_average = K.mean(disc_fake, axis=0)\n",
    "\n",
    "    def lossD(y_true, y_pred):\n",
    "        # epsilon=0.000001\n",
    "        # return -(K.mean(K.log(K.sigmoid(disc_real_average - disc_fake_average) + epsilon), axis=0) + K.mean(K.log(1 -\n",
    "        # K.sigmoid(disc_fake_average - disc_real_average) + epsilon), axis=0))\n",
    "        return K.mean(K.pow(disc_real_average - disc_fake_average - 1, 2), axis=0) + K.mean(K.pow(disc_fake_average - disc_real_average + 1, 2), axis=0)\n",
    "\n",
    "    def lossG(y_true, y_pred):\n",
    "        # epsilon=0.000001\n",
    "        # return -(K.mean(K.log(K.sigmoid(disc_fake_average - disc_real_average) + epsilon), axis=0) + K.mean(K.log(1 -\n",
    "        # K.sigmoid(disc_real_average - disc_fake_average) + epsilon), axis=0))\n",
    "        return K.mean(K.pow(disc_fake_average - disc_real_average - 1, 2), axis=0) + K.mean(K.pow(disc_real_average - disc_fake_average + 1, 2), axis=0)\n",
    "\n",
    "    # Build Optimizers\n",
    "    adamOP = Adam(lr=lr, beta_1=beta1)\n",
    "\n",
    "    # Build trainable generator and discriminator\n",
    "    disc_train = Model([noise_input, real_img], [disc_real, disc_fake])\n",
    "    gene.trainable = False\n",
    "    disc.trainable = True\n",
    "    disc_train.compile(optimizer=adamOP, loss=[lossD, None])\n",
    "    disc_train.summary()\n",
    "\n",
    "    gene_train = Model([noise_input, real_img], [disc_real, disc_fake])\n",
    "    gene.trainable = True\n",
    "    disc.trainable = False\n",
    "    gene_train.compile(optimizer=adamOP, loss=[lossG, None])\n",
    "    gene_train.summary()\n",
    "\n",
    "    # Start training\n",
    "    ideal_target = np.zeros((batch_size, 1), dtype=np.float32)\n",
    "\n",
    "    # Load data\n",
    "    images, names = read_data_dogimage.load_data()\n",
    "    images = images / 255 * 2 - 1\n",
    "\n",
    "    batch_num = int(len(images[0:]) // (batch_size * 2))\n",
    "\n",
    "    loss_d = []\n",
    "    loss_g = []\n",
    "\n",
    "    for epoch in np.arange(epochs):\n",
    "        np.random.shuffle(images)\n",
    "\n",
    "        print('current step: {:d} / {:d}, {:.2f}'.format((epoch + 1), epochs, (epoch + 1)/epochs))\n",
    "        start_time = time()\n",
    "\n",
    "        for batch_i in np.arange(0, batch_num):\n",
    "            batch = images[batch_i * (batch_size * 2): (batch_i + 1) * (batch_size * 2)]\n",
    "\n",
    "            # The result may be affected by the order or the frequency of training gene or disc per epoch.\n",
    "\n",
    "            batch_sec = batch[0 * batch_size: 1 * batch_size]\n",
    "            # noise = np.random.randn(batch_size, nz).astype(np.float32)\n",
    "            noise = truncnorm.rvs(-1.0, 1.0, size=(batch_size, nz)).astype(np.float32)\n",
    "            gene.trainable = True\n",
    "            disc.trainable = False\n",
    "            loss_g.append(gene_train.train_on_batch([noise, batch_sec], ideal_target))\n",
    "\n",
    "            batch_sec = batch[1 * batch_size: 2 * batch_size]\n",
    "            # noise = np.random.randn(batch_size, nz).astype(np.float32)\n",
    "            noise = truncnorm.rvs(-1.0, 1.0, size=(batch_size, nz)).astype(np.float32)\n",
    "            gene.trainable = False\n",
    "            disc.trainable = True\n",
    "            loss_d.append(disc_train.train_on_batch([noise, batch_sec], ideal_target))\n",
    "\n",
    "        print('lossG: {}, lossD: {}'.format(loss_g[-1][0], loss_d[-1][0]))\n",
    "        print('epoch time: {}\\n'.format(time() - start_time))\n",
    "\n",
    "    plt.plot(np.array(loss_g)[:, 0])\n",
    "    plt.plot(np.array(loss_d)[:, 0])\n",
    "    plt.legend(['generator', 'discriminator'])\n",
    "    plt.savefig('loss.png')\n",
    "\n",
    "    return gene\n",
    "\n",
    "\n",
    "class ImageGenerator:\n",
    "    act = 0\n",
    "\n",
    "    def __init__(self, gene):\n",
    "        self.gene = gene\n",
    "\n",
    "    def get_fake_img(self):\n",
    "        # noise = np.random.randn(1, nz).astype(np.float32)\n",
    "        noise = truncnorm.rvs(-1.0, 1.0, size=(1, nz)).astype(np.float32)\n",
    "        img = ((self.gene.predict(noise)[0].reshape((64, 64, 3)) + 1) / 2) * 255\n",
    "        self.act = (self.act + 1) % 10000\n",
    "        return Image.fromarray(img.astype('uint8'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gene = train()\n",
    "    I = ImageGenerator(gene)\n",
    "\n",
    "    z = zipfile.PyZipFile('images.zip', mode='w')\n",
    "    for k in range(10):\n",
    "        img = I.get_fake_img()\n",
    "        f = str(k) + '.png'\n",
    "        img.save(f, 'PNG')\n",
    "        z.write(f)\n",
    "        os.remove(f)\n",
    "        if k % 1000 == 0:\n",
    "            print(k)\n",
    "    z.close()\n",
    "    print('completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d247c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\rishu\\appdata\\local\\anaconda3\\envs\\rexxes\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement dogimage (from versions: none)\n",
      "ERROR: No matching distribution found for dogimage\n"
     ]
    }
   ],
   "source": [
    "pip install dogimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ce6c750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ zero_padding2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ZeroPadding2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_22          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_23          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_24          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6400</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,401</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_26 (\u001b[38;5;33mLeakyReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ zero_padding2d (\u001b[38;5;33mZeroPadding2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_22          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_27 (\u001b[38;5;33mLeakyReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_23          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_28 (\u001b[38;5;33mLeakyReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_24          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_29 (\u001b[38;5;33mLeakyReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6400\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m6,401\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">396,609</span> (1.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m396,609\u001b[0m (1.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">395,713</span> (1.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m395,713\u001b[0m (1.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"functional_31\" is incompatible with the layer: expected shape=(None, 64, 64, 3), found shape=(None, 128, 128, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • args=('<KerasTensor shape=(None, 128, 128, 3), dtype=float32, sparse=False, name=keras_tensor_290>',)\n  • kwargs={'mask': 'None'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 102\u001b[0m\n\u001b[0;32m     99\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m build_discriminator(img_shape)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Compile the models\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m gan \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(generator, discriminator, gan, epochs, batch_size, z_dim):\n",
      "Cell \u001b[1;32mIn[13], line 90\u001b[0m, in \u001b[0;36mcompile_models\u001b[1;34m(generator, discriminator)\u001b[0m\n\u001b[0;32m     88\u001b[0m img \u001b[38;5;241m=\u001b[39m generator(z)\n\u001b[0;32m     89\u001b[0m discriminator\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m validity \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m gan \u001b[38;5;241m=\u001b[39m Model(z, validity)\n\u001b[0;32m     92\u001b[0m gan\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mrelativistic_loss, optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlr, beta_1\u001b[38;5;241m=\u001b[39mbeta_1))\n",
      "File \u001b[1;32mc:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Rishu\\AppData\\Local\\anaconda3\\envs\\rexxes\\lib\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"functional_31\" is incompatible with the layer: expected shape=(None, 64, 64, 3), found shape=(None, 128, 128, 3)\u001b[0m\n\nArguments received by Sequential.call():\n  • args=('<KerasTensor shape=(None, 128, 128, 3), dtype=float32, sparse=False, name=keras_tensor_290>',)\n  • kwargs={'mask': 'None'}"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Sequential\n",
    "# dropout\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Conv2D, Conv2DTranspose, BatchNormalization, LeakyReLU, Activation, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Hyperparameters\n",
    "z_dim = 100  # Latent vector size\n",
    "img_shape = (64, 64, 3)  # Image shape\n",
    "batch_size = 64\n",
    "epochs = 10000\n",
    "lr = 0.0002\n",
    "beta_1 = 0.5\n",
    "\n",
    "# Load dataset (we'll use a resized version of the MNIST dataset for simplicity)\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "X_train = X_train / 127.5 - 1.0  # Normalize to [-1, 1]\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_train = np.repeat(X_train, 3, axis=-1)  # Convert to RGB images\n",
    "X_train = np.array([np.resize(img, (64, 64, 3)) for img in X_train])\n",
    "# Build the generator\n",
    "def generator(z_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=z_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))  # 64x64 images instead of 128x128\n",
    "    return model\n",
    "\n",
    "\n",
    "# Build the discriminator\n",
    "def build_discriminator(img_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size=3, strides=2, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "# Relativistic average least squares loss functions\n",
    "def relativistic_loss(real, fake):\n",
    "    return K.mean(K.pow(real - fake - 1, 2), axis=0) + K.mean(K.pow(fake - real + 1, 2), axis=0)\n",
    "\n",
    "# Compile the models\n",
    "def compile_models(generator, discriminator):\n",
    "    # Build the discriminator\n",
    "    discriminator.compile(loss=relativistic_loss, optimizer=Adam(learning_rate=lr, beta_1=beta_1), metrics=['accuracy'])\n",
    "\n",
    "    # Build the GAN\n",
    "    z = Input(shape=(z_dim,))\n",
    "    img = generator(z)\n",
    "    discriminator.trainable = False\n",
    "    validity = discriminator(img)\n",
    "    gan = Model(z, validity)\n",
    "    gan.compile(loss=relativistic_loss, optimizer=Adam(learning_rate=lr, beta_1=beta_1))\n",
    "\n",
    "    return gan\n",
    "\n",
    "\n",
    "# Create the models\n",
    "generator = build_generator(z_dim)\n",
    "discriminator = build_discriminator(img_shape)\n",
    "\n",
    "# Compile the models\n",
    "gan = compile_models(generator, discriminator)\n",
    "\n",
    "# Training loop\n",
    "def train(generator, discriminator, gan, epochs, batch_size, z_dim):\n",
    "    half_batch = batch_size // 2\n",
    "    for epoch in range(epochs):\n",
    "        # Select a random half-batch of real images\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        real_images = X_train[idx]\n",
    "\n",
    "        # Generate a half-batch of fake images\n",
    "        noise = np.random.normal(0, 1, (half_batch, z_dim))\n",
    "        fake_images = generator.predict(noise)\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss_real = discriminator.train_on_batch(real_images, np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_images, np.zeros((half_batch, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train the generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "        # Print the progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]}] [G loss: {g_loss}]\")\n",
    "\n",
    "        # Save generated images at intervals\n",
    "        if epoch % 1000 == 0:\n",
    "            save_imgs(epoch, generator)\n",
    "\n",
    "# Function to save generated images\n",
    "def save_imgs(epoch, generator):\n",
    "    noise = np.random.normal(0, 1, (25, z_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images to [0, 1]\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    # Plot the images\n",
    "    fig, axs = plt.subplots(5, 5)\n",
    "    cnt = 0\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            axs[i, j].imshow(gen_imgs[cnt])\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(f\"images_{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Start training\n",
    "train(generator, discriminator, gan, epochs, batch_size, z_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2233db15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rexxes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
