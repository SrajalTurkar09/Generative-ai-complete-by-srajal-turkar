{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "d9vLvQcsBw1b",
        "outputId": "01451ee7-2450-46ec-f0bf-5c4886e6f184"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling DeformableConv2D.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'deformable_conv2d_6' (of type DeformableConv2D). Either the `DeformableConv2D.call()` method is incorrect, or you need to implement the `DeformableConv2D.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 64 and 32 for '{{node add}} = AddV2[T=DT_FLOAT](Tile, conv2d_16_1/add)' with input shapes: [?,64,64,2], [?,32,32,18].\u001b[0m\n\nArguments received by DeformableConv2D.call():\n  • args=('<KerasTensor shape=(None, 64, 64, 64), dtype=float32, sparse=False, name=keras_tensor_20>',)\n  • kwargs=<class 'inspect._empty'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-705f5b7b3ccb>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;31m# Step 4: Compile the Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-705f5b7b3ccb>\u001b[0m in \u001b[0;36mbuild_generator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Deformable Convolution layer to further downsample with deformable convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeformableConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Upsampling via transposed convolution (deconvolution) to generate larger feature maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-705f5b7b3ccb>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# Apply offsets to the grid, ensuring the dimensions match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0moffset_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffset\u001b[0m  \u001b[0;31m# Shape becomes [batch_size, height, width, 2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Sample the input image based on the adjusted grid (deformable sampling)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling DeformableConv2D.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'deformable_conv2d_6' (of type DeformableConv2D). Either the `DeformableConv2D.call()` method is incorrect, or you need to implement the `DeformableConv2D.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 64 and 32 for '{{node add}} = AddV2[T=DT_FLOAT](Tile, conv2d_16_1/add)' with input shapes: [?,64,64,2], [?,32,32,18].\u001b[0m\n\nArguments received by DeformableConv2D.call():\n  • args=('<KerasTensor shape=(None, 64, 64, 64), dtype=float32, sparse=False, name=keras_tensor_20>',)\n  • kwargs=<class 'inspect._empty'>"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Define the Deformable Convolution Layer\n",
        "# This layer applies a deformable convolution operation with learnable offsets to input images\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class DeformableConv2D(layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, strides=1, padding='same', **kwargs):\n",
        "        super(DeformableConv2D, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size[0]\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "\n",
        "        # Convolution layer to generate offset values (how the kernel moves)\n",
        "        # Set dilation_rate to 1 when strides > 1\n",
        "        dilation_rate = 1 if strides > 1 else strides\n",
        "        self.offset_conv = layers.Conv2D(2 * self.kernel_size * self.kernel_size, (self.kernel_size, self.kernel_size), padding=padding, strides=strides, dilation_rate=dilation_rate)\n",
        "\n",
        "        # Convolution layer to generate modulation values (how much we scale the pixels)\n",
        "        # Set dilation_rate to 1 when strides > 1\n",
        "        self.modulation_conv = layers.Conv2D(self.kernel_size * self.kernel_size, (self.kernel_size, self.kernel_size), padding=padding, strides=strides, dilation_rate=dilation_rate)\n",
        "\n",
        "        # Final convolutional layer to generate the output features\n",
        "        self.main_conv = layers.Conv2D(filters, (self.kernel_size, self.kernel_size), padding=padding, strides=strides)\n",
        "\n",
        "    # ... (rest of the code remains the same) ...\n",
        "    # ... (rest of the code remains the same) ...\n",
        "    def build(self, input_shape):\n",
        "        # This method is used to initialize the layer's state\n",
        "        self.input_shape = input_shape  # Save the input shape for future use\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Generate offsets for the convolution grid\n",
        "        offset = self.offset_conv(inputs)\n",
        "\n",
        "        # Generate modulation weights\n",
        "        modulation = tf.sigmoid(self.modulation_conv(inputs))  # Sigmoid to get values between 0 and 1\n",
        "\n",
        "        # Create a grid of indices based on the image dimensions\n",
        "        grid = tf.meshgrid(\n",
        "            tf.range(tf.shape(inputs)[1]), tf.range(tf.shape(inputs)[2]), indexing='ij'\n",
        "        )\n",
        "        grid = tf.stack(grid, axis=-1)  # Shape becomes [height, width, 2]\n",
        "        grid = tf.cast(grid, tf.float32)  # Cast grid indices to float32\n",
        "\n",
        "        # Expand grid and offset to match the batch size and input dimensions\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        grid = tf.tile(tf.expand_dims(grid, 0), [batch_size, 1, 1, 1])  # Shape becomes [batch_size, height, width, 2]\n",
        "\n",
        "        # Apply offsets to the grid, ensuring the dimensions match\n",
        "        offset_grid = grid + offset  # Shape becomes [batch_size, height, width, 2]\n",
        "\n",
        "        # Sample the input image based on the adjusted grid (deformable sampling)\n",
        "        sampled_inputs = self._deformable_sample(inputs, offset_grid)\n",
        "\n",
        "        # Scale the sampled inputs based on the modulation values\n",
        "        sampled_inputs = sampled_inputs * modulation\n",
        "\n",
        "        # Apply the final convolution layer\n",
        "        return self.main_conv(sampled_inputs)\n",
        "\n",
        "    def _deformable_sample(self, inputs, offset_grid):\n",
        "        \"\"\"Function to sample the inputs based on offset grid (not implemented here).\"\"\"\n",
        "        # This is a placeholder function for the deformable sampling process.\n",
        "        # In practice, you would need to implement or use a function to sample the input image\n",
        "        # based on the offset_grid. This is typically done using bilinear interpolation.\n",
        "        # For now, we'll assume that you have a working method for sampling.\n",
        "        return inputs  # Placeholder (in practice, you will sample using the offset_grid)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Build the Generator Model\n",
        "# The generator creates fake images from random noise input\n",
        "def build_generator():\n",
        "    inputs = layers.Input(shape=(128, 128, 3))  # Input is an image of size 128x128 with 3 channels (RGB)\n",
        "\n",
        "    # Convolutional layer to downsample the input\n",
        "    x = layers.Conv2D(64, (3, 3), strides=2, padding='same', activation='relu')(inputs)\n",
        "    x = layers.BatchNormalization()(x)  # Apply batch normalization for stable training\n",
        "\n",
        "    # Deformable Convolution layer to further downsample with deformable convolution\n",
        "    x = DeformableConv2D(128, (3, 3), strides=2)(x)\n",
        "\n",
        "    # Upsampling via transposed convolution (deconvolution) to generate larger feature maps\n",
        "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Final convolution layer to generate the output image\n",
        "    x = layers.Conv2D(3, (3, 3), padding='same', activation='tanh')(x)  # Output image with 3 channels (RGB)\n",
        "\n",
        "    return Model(inputs, x, name=\"Generator\")\n",
        "\n",
        "\n",
        "# Step 3: Build the Discriminator Model\n",
        "# The discriminator distinguishes between real and fake images\n",
        "def build_discriminator():\n",
        "    inputs = layers.Input(shape=(128, 128, 3))  # Input is an image of size 128x128 with 3 channels (RGB)\n",
        "\n",
        "    # Convolutional layers to extract features from the image\n",
        "    x = layers.Conv2D(64, (3, 3), strides=2, padding='same', activation='relu')(inputs)\n",
        "    x = layers.BatchNormalization()(x)  # Batch normalization to stabilize training\n",
        "\n",
        "    x = layers.Conv2D(128, (3, 3), strides=2, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Flatten the output and apply a fully connected layer for classification\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)  # Sigmoid activation to classify between real (1) and fake (0)\n",
        "\n",
        "    return Model(inputs, x, name=\"Discriminator\")\n",
        "\n",
        "\n",
        "# Step 4: Compile the Models\n",
        "generator = build_generator()  # Create the generator\n",
        "discriminator = build_discriminator()  # Create the discriminator\n",
        "\n",
        "# Compile the discriminator with Adam optimizer and binary crossentropy loss\n",
        "discriminator.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy')\n",
        "discriminator.trainable = False  # Freeze discriminator during generator training\n",
        "\n",
        "# Create the GAN model by combining the generator and discriminator\n",
        "gan_input = layers.Input(shape=(128, 128, 3))  # Input is a random noise vector of size 128x128x3\n",
        "generated_image = generator(gan_input)  # Pass input noise through the generator\n",
        "gan_output = discriminator(generated_image)  # Classify the generated image using the discriminator\n",
        "gan = Model(gan_input, gan_output)\n",
        "\n",
        "# Compile the GAN model\n",
        "gan.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy')\n",
        "\n",
        "\n",
        "# Step 5: CIFAR-10 Dataset Preparation\n",
        "# Load CIFAR-10 dataset, preprocess and normalize the images\n",
        "def preprocess_images(image):\n",
        "    # Resize the image to 128x128\n",
        "    image = tf.image.resize(image, [128, 128])\n",
        "    return (image - 127.5) / 127.5  # Normalize the pixel values to [-1, 1]\n",
        "\n",
        "# Load the CIFAR-10 dataset from TensorFlow's built-in datasets\n",
        "(x_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Preprocess and normalize the images\n",
        "x_train = preprocess_images(x_train)\n",
        "\n",
        "# Convert the images into a TensorFlow dataset and batch them\n",
        "dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "dataset = dataset.shuffle(1000).batch(32)  # Shuffle and batch the dataset\n",
        "\n",
        "\n",
        "# Step 6: Training Loop\n",
        "# Function to train the GAN\n",
        "def train(generator, discriminator, gan, dataset, epochs=50):\n",
        "    for epoch in range(epochs):\n",
        "        for real_images in dataset:\n",
        "            batch_size = real_images.shape[0]  # Get the batch size\n",
        "\n",
        "            # Generate fake images using the generator\n",
        "            noise = tf.random.normal((batch_size, 128, 128, 3))  # Random noise for generator input\n",
        "            fake_images = generator(noise)  # Generate fake images from the noise\n",
        "\n",
        "            # Create labels for real and fake images (1 for real, 0 for fake)\n",
        "            real_labels = tf.ones((batch_size, 1))  # Label for real images\n",
        "            fake_labels = tf.zeros((batch_size, 1))  # Label for fake images\n",
        "\n",
        "            # Train the discriminator with real images\n",
        "            discriminator_loss_real = discriminator.train_on_batch(real_images, real_labels)\n",
        "\n",
        "            # Train the discriminator with fake images\n",
        "            discriminator_loss_fake = discriminator.train_on_batch(fake_images, fake_labels)\n",
        "\n",
        "            # Average discriminator loss\n",
        "            discriminator_loss = 0.5 * (discriminator_loss_real + discriminator_loss_fake)\n",
        "\n",
        "            # Train the generator via the GAN (freeze discriminator)\n",
        "            generator_loss = gan.train_on_batch(noise, real_labels)\n",
        "\n",
        "        # Print the loss values at each epoch\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Discriminator Loss: {discriminator_loss:.4f}, Generator Loss: {generator_loss:.4f}\")\n",
        "\n",
        "\n",
        "# Step 7: Generate and Visualize Results\n",
        "# After training, this function visualizes generated images\n",
        "def generate_images(generator, num_images=5):\n",
        "    noise = tf.random.normal((num_images, 128, 128, 3))  # Generate random noise\n",
        "    generated_images = generator(noise)  # Generate images from noise\n",
        "\n",
        "    # Plot the generated images\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i+1)\n",
        "        plt.imshow((generated_images[i] + 1) / 2)  # Rescale image to [0, 1] for display\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Step 8: Run Training and Visualization\n",
        "train(generator, discriminator, gan, dataset, epochs=50)  # Train the GAN for 50 epochs\n",
        "generate_images(generator, num_images=5)  # Generate and display 5 images after training\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Define the Deformable Convolution Layer\n",
        "class DeformableConv2D(layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, strides=1, padding='same', **kwargs):\n",
        "        super(DeformableConv2D, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size[0]\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "\n",
        "        # Convolution layer to generate offset values (how the kernel moves)\n",
        "        self.offset_conv = layers.Conv2D(2 * self.kernel_size * self.kernel_size, (self.kernel_size, self.kernel_size), padding=padding, strides=strides)\n",
        "\n",
        "        # Convolution layer to generate modulation values (how much we scale the pixels)\n",
        "        self.modulation_conv = layers.Conv2D(self.kernel_size * self.kernel_size, (self.kernel_size, self.kernel_size), padding=padding, strides=strides)\n",
        "\n",
        "        # Final convolutional layer to generate the output features\n",
        "        self.main_conv = layers.Conv2D(filters, (self.kernel_size, self.kernel_size), padding=padding, strides=strides)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_shape = input_shape  # Save the input shape for future use\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Generate offsets for the convolution grid\n",
        "        offset = self.offset_conv(inputs)  # Shape: [batch_size, height, width, 2 * kernel_size^2]\n",
        "\n",
        "        # Generate modulation weights\n",
        "        modulation = tf.sigmoid(self.modulation_conv(inputs))  # Shape: [batch_size, height, width, kernel_size^2]\n",
        "\n",
        "        # Create a grid of indices based on the image dimensions\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        height, width = inputs.shape[1], inputs.shape[2]\n",
        "        grid = tf.meshgrid(\n",
        "            tf.range(height), tf.range(width), indexing='ij'\n",
        "        )\n",
        "        grid = tf.stack(grid, axis=-1)  # Shape becomes [height, width, 2]\n",
        "        grid = tf.cast(grid, tf.float32)  # Cast grid indices to float32\n",
        "\n",
        "        # Expand grid to match batch size: Shape becomes [batch_size, height, width, 2]\n",
        "        grid = tf.tile(tf.expand_dims(grid, 0), [batch_size, 1, 1, 1])\n",
        "\n",
        "        # Reshape and apply offsets to the grid, ensuring the dimensions match\n",
        "        offset = tf.reshape(offset, [batch_size, height, width, 2, self.kernel_size, self.kernel_size])  # Shape: [batch_size, height, width, 2, kernel_size, kernel_size]\n",
        "        offset_grid = grid[..., None, :] + offset  # Shape becomes [batch_size, height, width, kernel_size, kernel_size, 2]\n",
        "\n",
        "        # Sample the input image based on the adjusted grid (deformable sampling)\n",
        "        sampled_inputs = self._deformable_sample(inputs, offset_grid)\n",
        "\n",
        "        # Scale the sampled inputs based on the modulation values\n",
        "        sampled_inputs = sampled_inputs * modulation\n",
        "\n",
        "        # Apply the final convolution layer\n",
        "        return self.main_conv(sampled_inputs)\n",
        "\n",
        "    def _deformable_sample(self, inputs, offset_grid):\n",
        "        \"\"\"Function to sample the inputs based on offset grid (not implemented here).\"\"\"\n",
        "        # This is a placeholder function for the deformable sampling process.\n",
        "        # In practice, you would need to implement or use a function to sample the input image\n",
        "        # based on the offset_grid. This is typically done using bilinear interpolation.\n",
        "        # For now, we'll assume that you have a working method for sampling.\n",
        "        return inputs  # Placeholder (in practice, you will sample using the offset_grid)\n",
        "\n",
        "\n",
        "# Step 2: Build the Generator Model\n",
        "def build_generator():\n",
        "    inputs = layers.Input(shape=(128, 128, 3))  # Input is an image of size 128x128 with 3 channels (RGB)\n",
        "\n",
        "    # Convolutional layer to downsample the input\n",
        "    x = layers.Conv2D(64, (3, 3), strides=2, padding='same', activation='relu')(inputs)\n",
        "    x = layers.BatchNormalization()(x)  # Apply batch normalization for stable training\n",
        "\n",
        "    # Deformable Convolution layer to further downsample with deformable convolution\n",
        "    x = DeformableConv2D(128, (3, 3), strides=2)(x)\n",
        "\n",
        "    # Upsampling via transposed convolution (deconvolution) to generate larger feature maps\n",
        "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Final convolution layer to generate the output image\n",
        "    x = layers.Conv2D(3, (3, 3), padding='same', activation='tanh')(x)  # Output image with 3 channels (RGB)\n",
        "\n",
        "    return Model(inputs, x, name=\"Generator\")\n",
        "\n",
        "\n",
        "# Step 3: Build the Discriminator Model\n",
        "def build_discriminator():\n",
        "    inputs = layers.Input(shape=(128, 128, 3))  # Input is an image of size 128x128 with 3 channels (RGB)\n",
        "\n",
        "    # Convolutional layers to extract features from the image\n",
        "    x = layers.Conv2D(64, (3, 3), strides=2, padding='same', activation='relu')(inputs)\n",
        "    x = layers.BatchNormalization()(x)  # Batch normalization to stabilize training\n",
        "\n",
        "    x = layers.Conv2D(128, (3, 3), strides=2, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Flatten the output and apply a fully connected layer for classification\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)  # Sigmoid activation to classify between real (1) and fake (0)\n",
        "\n",
        "    return Model(inputs, x, name=\"Discriminator\")\n",
        "\n",
        "\n",
        "# Step 4: Compile the Models\n",
        "generator = build_generator()  # Create the generator\n",
        "discriminator = build_discriminator()  # Create the discriminator\n",
        "\n",
        "# Compile the discriminator with Adam optimizer and binary crossentropy loss\n",
        "discriminator.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy')\n",
        "discriminator.trainable = False  # Freeze discriminator during generator training\n",
        "\n",
        "# Create the GAN model by combining the generator and discriminator\n",
        "gan_input = layers.Input(shape=(128, 128, 3))  # Input is a random noise vector of size 128x128x3\n",
        "generated_image = generator(gan_input)  # Pass input noise through the generator\n",
        "gan_output = discriminator(generated_image)  # Classify the generated image using the discriminator\n",
        "gan = Model(gan_input, gan_output)\n",
        "\n",
        "# Compile the GAN model\n",
        "gan.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy')\n",
        "\n",
        "\n",
        "# Step 5: CIFAR-10 Dataset Preparation\n",
        "def preprocess_images(image):\n",
        "    # Resize the image to 128x128\n",
        "    image = tf.image.resize(image, [128, 128])\n",
        "    return (image - 127.5) / 127.5  # Normalize the pixel values to [-1, 1]\n",
        "\n",
        "# Load the CIFAR-10 dataset from TensorFlow's built-in datasets\n",
        "(x_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Preprocess and normalize the images\n",
        "x_train = preprocess_images(x_train)\n",
        "\n",
        "# Convert the images into a TensorFlow dataset and batch them\n",
        "dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "dataset = dataset.shuffle(1000).batch(32)  # Shuffle and batch the dataset\n",
        "\n",
        "\n",
        "# Step 6: Training Loop\n",
        "def train(generator, discriminator, gan, dataset, epochs=50):\n",
        "    for epoch in range(epochs):\n",
        "        for real_images in dataset:\n",
        "            batch_size = real_images.shape[0]  # Get the batch size\n",
        "\n",
        "            # Generate fake images using the generator\n",
        "            noise = tf.random.normal((batch_size, 128, 128, 3))  # Random noise for generator input\n",
        "            fake_images = generator(noise)  # Generate fake images from the noise\n",
        "\n",
        "            # Create labels for real and fake images (1 for real, 0 for fake)\n",
        "            real_labels = tf.ones((batch_size, 1))  # Label for real images\n",
        "            fake_labels = tf.zeros((batch_size, 1))  # Label for fake images\n",
        "\n",
        "            # Train the discriminator\n",
        "            with tf.GradientTape() as tape:\n",
        "                real_output = discriminator(real_images)  # Get discriminator output for real images\n",
        "                fake_output = discriminator(fake_images)  # Get discriminator output for fake images\n",
        "                d_loss_real = tf.keras.losses.binary_crossentropy(real_labels, real_output)\n",
        "                d_loss_fake = tf.keras.losses.binary_crossentropy(fake_labels, fake_output)\n",
        "                d_loss = d_loss_real + d_loss_fake  # Total discriminator loss\n",
        "            grads = tape.gradient(d_loss, discriminator.trainable_variables)  # Compute gradients\n",
        "            discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))  # Apply gradients\n",
        "\n",
        "            # Train the generator (via the GAN model)\n",
        "            with tf.GradientTape() as tape:\n",
        "                fake_images = generator(noise)  # Generate fake images\n",
        "                fake_output = discriminator(fake_images)  # Get discriminator output for fake images\n",
        "                g_loss = tf.keras.losses.binary_crossentropy(real_labels, fake_output)  # Generator loss\n",
        "            grads = tape.gradient(g_loss, generator.trainable_variables)  # Compute gradients\n",
        "            gan.optimizer.apply_gradients(zip(grads, generator.trainable_variables))  # Apply gradients\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs} | D Loss: {d_loss.numpy()} | G Loss: {g_loss.numpy()}')\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            generate_images(generator)\n",
        "\n",
        "# Generate images function for visualization\n",
        "def generate_images(generator, num_images=10):\n",
        "    noise = tf.random.normal((num_images, 128, 128, 3))  # Random noise\n",
        "    generated_images = generator(noise)  # Generate images from noise\n",
        "\n",
        "    # Rescale the images to [0, 255] range for visualization\n",
        "    generated_images = (generated_images + 1) * 127.5  # Rescale from [-1, 1] to [0, 255]\n",
        "\n",
        "    # Display the generated images\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i+1)\n",
        "        plt.imshow(generated_images[i].numpy().astype(\"uint8\"))\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Step 7: Start training the GAN\n",
        "train(generator, discriminator, gan, dataset, epochs=50)\n",
        "\n",
        "# Generate and visualize some fake images after training\n",
        "generate_images(generator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "qSbr4iRVFblQ",
        "outputId": "9dd60f9d-da98-4f21-9e22-6933ed51fc5a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling DeformableConv2D.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'deformable_conv2d_8' (of type DeformableConv2D). Either the `DeformableConv2D.call()` method is incorrect, or you need to implement the `DeformableConv2D.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 64 and 2 for '{{node add}} = AddV2[T=DT_FLOAT](strided_slice_1, Reshape)' with input shapes: [?,64,64,1,2], [?,64,64,2,3,3].\u001b[0m\n\nArguments received by DeformableConv2D.call():\n  • args=('<KerasTensor shape=(None, 64, 64, 64), dtype=float32, sparse=False, name=keras_tensor_26>',)\n  • kwargs=<class 'inspect._empty'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-112f6ca4b12e>\u001b[0m in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m# Step 4: Compile the Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-112f6ca4b12e>\u001b[0m in \u001b[0;36mbuild_generator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# Deformable Convolution layer to further downsample with deformable convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeformableConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;31m# Upsampling via transposed convolution (deconvolution) to generate larger feature maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-112f6ca4b12e>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Reshape and apply offsets to the grid, ensuring the dimensions match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: [batch_size, height, width, 2, kernel_size, kernel_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0moffset_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffset\u001b[0m  \u001b[0;31m# Shape becomes [batch_size, height, width, kernel_size, kernel_size, 2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Sample the input image based on the adjusted grid (deformable sampling)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling DeformableConv2D.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'deformable_conv2d_8' (of type DeformableConv2D). Either the `DeformableConv2D.call()` method is incorrect, or you need to implement the `DeformableConv2D.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 64 and 2 for '{{node add}} = AddV2[T=DT_FLOAT](strided_slice_1, Reshape)' with input shapes: [?,64,64,1,2], [?,64,64,2,3,3].\u001b[0m\n\nArguments received by DeformableConv2D.call():\n  • args=('<KerasTensor shape=(None, 64, 64, 64), dtype=float32, sparse=False, name=keras_tensor_26>',)\n  • kwargs=<class 'inspect._empty'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Define the Deformable Convolution Layer\n",
        "class DeformableConv2D(layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, strides=1, padding='same', **kwargs):\n",
        "        super(DeformableConv2D, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size[0]\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "\n",
        "        # Convolution layer to generate offset values (how the kernel moves)\n",
        "        self.offset_conv = layers.Conv2D(2 * self.kernel_size * self.kernel_size, (self.kernel_size, self.kernel_size), padding=padding, strides=strides)\n",
        "\n",
        "        # Convolution layer to generate modulation values (how much we scale the pixels)\n",
        "        self.modulation_conv = layers.Conv2D(self.kernel_size * self.kernel_size, (self.kernel_size, self.kernel_size), padding=padding, strides=strides)\n",
        "\n",
        "        # Final convolutional layer to generate the output features\n",
        "        self.main_conv = layers.Conv2D(filters, (self.kernel_size, self.kernel_size), padding=padding, strides=strides)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_shape = input_shape  # Save the input shape for future use\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Generate offsets for the convolution grid\n",
        "        offset = self.offset_conv(inputs)  # Shape: [batch_size, height, width, 2 * kernel_size^2]\n",
        "\n",
        "        # Generate modulation weights\n",
        "        modulation = tf.sigmoid(self.modulation_conv(inputs))  # Shape: [batch_size, height, width, kernel_size^2]\n",
        "\n",
        "        # Create a grid of indices based on the image dimensions\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        height, width = inputs.shape[1], inputs.shape[2]\n",
        "        grid = tf.meshgrid(\n",
        "            tf.range(height), tf.range(width), indexing='ij'\n",
        "        )\n",
        "        grid = tf.stack(grid, axis=-1)  # Shape becomes [height, width, 2]\n",
        "        grid = tf.cast(grid, tf.float32)  # Cast grid indices to float32\n",
        "\n",
        "        # Expand grid to match batch size: Shape becomes [batch_size, height, width, 2]\n",
        "        grid = tf.tile(tf.expand_dims(grid, 0), [batch_size, 1, 1, 1])\n",
        "\n",
        "        # Reshape and apply offsets to the grid, ensuring the dimensions match\n",
        "        offset = tf.reshape(offset, [batch_size, height, width, self.kernel_size, self.kernel_size, 2])  # Shape: [batch_size, height, width, kernel_size, kernel_size, 2]\n",
        "        grid_expanded = grid[..., None, None, :]  # Shape: [batch_size, height, width, 1, 1, 2]\n",
        "\n",
        "        offset_grid = grid_expanded + offset  # Shape becomes [batch_size, height, width, kernel_size, kernel_size, 2]\n",
        "\n",
        "        # Sample the input image based on the adjusted grid (deformable sampling)\n",
        "        sampled_inputs = self._deformable_sample(inputs, offset_grid)\n",
        "\n",
        "        # Scale the sampled inputs based on the modulation values\n",
        "        sampled_inputs = sampled_inputs * modulation\n",
        "\n",
        "        # Apply the final convolution layer\n",
        "        return self.main_conv(sampled_inputs)\n",
        "\n",
        "    def _deformable_sample(self, inputs, offset_grid):\n",
        "        \"\"\"Function to sample the inputs based on offset grid (not implemented here).\"\"\"\n",
        "        # This is a placeholder function for the deformable sampling process.\n",
        "        # In practice, you would need to implement or use a function to sample the input image\n",
        "        # based on the offset_grid. This is typically done using bilinear interpolation.\n",
        "        # For now, we'll assume that you have a working method for sampling.\n",
        "        return inputs  # Placeholder (in practice, you will sample using the offset_grid)\n",
        "\n",
        "\n",
        "# Step 2: Build the Generator Model\n",
        "def build_generator():\n",
        "    inputs = layers.Input(shape=(128, 128, 3))  # Input is an image of size 128x128 with 3 channels (RGB)\n",
        "\n",
        "    # Convolutional layer to downsample the input\n",
        "    x = layers.Conv2D(64, (3, 3), strides=2, padding='same', activation='relu')(inputs)\n",
        "    x = layers.BatchNormalization()(x)  # Apply batch normalization for stable training\n",
        "\n",
        "    # Deformable Convolution layer to further downsample with deformable convolution\n",
        "    x = DeformableConv2D(128, (3, 3), strides=2)(x)\n",
        "\n",
        "    # Upsampling via transposed convolution (deconvolution) to generate larger feature maps\n",
        "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Final convolution layer to generate the output image\n",
        "    x = layers.Conv2D(3, (3, 3), padding='same', activation='tanh')(x)  # Output image with 3 channels (RGB)\n",
        "\n",
        "    return Model(inputs, x, name=\"Generator\")\n",
        "\n",
        "\n",
        "# Step 3: Build the Discriminator Model\n",
        "def build_discriminator():\n",
        "    inputs = layers.Input(shape=(128, 128, 3))  # Input is an image of size 128x128 with 3 channels (RGB)\n",
        "\n",
        "    # Convolutional layers to extract features from the image\n",
        "    x = layers.Conv2D(64, (3, 3), strides=2, padding='same', activation='relu')(inputs)\n",
        "    x = layers.BatchNormalization()(x)  # Batch normalization to stabilize training\n",
        "\n",
        "    x = layers.Conv2D(128, (3, 3), strides=2, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Flatten the output and apply a fully connected layer for classification\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)  # Sigmoid activation to classify between real (1) and fake (0)\n",
        "\n",
        "    return Model(inputs, x, name=\"Discriminator\")\n",
        "\n",
        "\n",
        "# Step 4: Compile the Models\n",
        "generator = build_generator()  # Create the generator\n",
        "discriminator = build_discriminator()  # Create the discriminator\n",
        "\n",
        "# Compile the discriminator with Adam optimizer and binary crossentropy loss\n",
        "discriminator.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy')\n",
        "discriminator.trainable = False  # Freeze discriminator during generator training\n",
        "\n",
        "# Create the GAN model by combining the generator and discriminator\n",
        "gan_input = layers.Input(shape=(128, 128, 3))  # Input is a random noise vector of size 128x128x3\n",
        "generated_image = generator(gan_input)  # Pass input noise through the generator\n",
        "gan_output = discriminator(generated_image)  # Classify the generated image using the discriminator\n",
        "gan = Model(gan_input, gan_output)\n",
        "\n",
        "# Compile the GAN model\n",
        "gan.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy')\n",
        "\n",
        "\n",
        "# Step 5: CIFAR-10 Dataset Preparation\n",
        "def preprocess_images(image):\n",
        "    # Resize the image to 128x128\n",
        "    image = tf.image.resize(image, [128, 128])\n",
        "    return (image - 127.5) / 127.5  # Normalize the pixel values to [-1, 1]\n",
        "\n",
        "# Load the CIFAR-10 dataset from TensorFlow's built-in datasets\n",
        "(x_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Preprocess and normalize the images\n",
        "x_train = preprocess_images(x_train)\n",
        "\n",
        "# Convert the images into a TensorFlow dataset and batch them\n",
        "dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "dataset = dataset.shuffle(1000).batch(32)  # Shuffle and batch the dataset\n",
        "\n",
        "\n",
        "# Step 6: Training Loop\n",
        "def train(generator, discriminator, gan, dataset, epochs=50):\n",
        "    for epoch in range(epochs):\n",
        "        for real_images in dataset:\n",
        "            batch_size = real_images.shape[0]  # Get the batch size\n",
        "\n",
        "            # Generate fake images using the generator\n",
        "            noise = tf.random.normal((batch_size, 128, 128, 3))  # Random noise for generator input\n",
        "            fake_images = generator(noise)  # Generate fake images from the noise\n",
        "\n",
        "            # Create labels for real and fake images (1 for real, 0 for fake)\n",
        "            real_labels = tf.ones((batch_size, 1))  # Label for real images\n",
        "            fake_labels = tf.zeros((batch_size, 1))  # Label for fake images\n",
        "\n",
        "            # Train the discriminator\n",
        "            with tf.GradientTape() as tape:\n",
        "                real_output = discriminator(real_images)  # Get discriminator output for real images\n",
        "                fake_output = discriminator(fake_images)  # Get discriminator output for fake images\n",
        "                d_loss_real = tf.keras.losses.binary_crossentropy(real_labels, real_output)\n",
        "                d_loss_fake = tf.keras.losses.binary_crossentropy(fake_labels, fake_output)\n",
        "                d_loss = d_loss_real + d_loss_fake  # Total discriminator loss\n",
        "            grads = tape.gradient(d_loss, discriminator.trainable_variables)  # Compute gradients\n",
        "            discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))  # Apply gradients\n",
        "\n",
        "            # Train the generator (via the GAN model)\n",
        "            with tf.GradientTape() as tape:\n",
        "                fake_images = generator(noise)  # Generate fake images\n",
        "                fake_output = discriminator(fake_images)  # Get discriminator output for fake images\n",
        "                g_loss = tf.keras.losses.binary_crossentropy(real_labels, fake_output)  # Generator loss\n",
        "            grads = tape.gradient(g_loss, generator.trainable_variables)  # Compute gradients\n",
        "            gan.optimizer.apply_gradients(zip(grads, generator.trainable_variables))  # Apply gradients\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs} | D Loss: {d_loss.numpy()} | G Loss: {g_loss.numpy()}')\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            generate_images(generator)\n",
        "\n",
        "\n",
        "# Generate images function for visualization\n",
        "def generate_images(generator, num_images=10):\n",
        "    noise = tf.random.normal((num_images, 128, 128, 3))  # Random noise\n",
        "    generated_images = generator(noise)  # Generate images from noise\n",
        "\n",
        "    # Rescale the images to [0, 255] range for visualization\n",
        "    generated_images = (generated_images + 1) * 127.5  # Rescale from [-1, 1] to [0, 255]\n",
        "\n",
        "    # Display the generated images\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i+1)\n",
        "        plt.imshow(generated_images[i].numpy().astype(\"uint8\"))\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Step 7: Start training the GAN\n",
        "train(generator, discriminator, gan, dataset, epochs=50)\n",
        "\n",
        "# Generate and visualize some fake images after training\n",
        "generate_images(generator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "zbobWm2bPDH_",
        "outputId": "52fb4072-4b2b-4854-a791-6640eb9c1839"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling DeformableConv2D.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'deformable_conv2d_9' (of type DeformableConv2D). Either the `DeformableConv2D.call()` method is incorrect, or you need to implement the `DeformableConv2D.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 64 and 32 for '{{node mul}} = Mul[T=DT_FLOAT](Placeholder, Sigmoid)' with input shapes: [?,64,64,64], [?,32,32,9].\u001b[0m\n\nArguments received by DeformableConv2D.call():\n  • args=('<KerasTensor shape=(None, 64, 64, 64), dtype=float32, sparse=False, name=keras_tensor_29>',)\n  • kwargs=<class 'inspect._empty'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c528579b92fe>\u001b[0m in \u001b[0;36m<cell line: 107>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# Step 4: Compile the Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-c528579b92fe>\u001b[0m in \u001b[0;36mbuild_generator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Deformable Convolution layer to further downsample with deformable convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeformableConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Upsampling via transposed convolution (deconvolution) to generate larger feature maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-c528579b92fe>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Scale the sampled inputs based on the modulation values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0msampled_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled_inputs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmodulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Apply the final convolution layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling DeformableConv2D.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'deformable_conv2d_9' (of type DeformableConv2D). Either the `DeformableConv2D.call()` method is incorrect, or you need to implement the `DeformableConv2D.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 64 and 32 for '{{node mul}} = Mul[T=DT_FLOAT](Placeholder, Sigmoid)' with input shapes: [?,64,64,64], [?,32,32,9].\u001b[0m\n\nArguments received by DeformableConv2D.call():\n  • args=('<KerasTensor shape=(None, 64, 64, 64), dtype=float32, sparse=False, name=keras_tensor_29>',)\n  • kwargs=<class 'inspect._empty'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Define the Deformable Convolution Layer\n",
        "class DeformableConv2D(layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, strides=1, padding='same', **kwargs):\n",
        "        super(DeformableConv2D, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size[0]\n",
        "        self.strides = strides\n",
        "        self.padding = padding\n",
        "\n",
        "        # Convolution layer to generate offset values (how the kernel moves)\n",
        "        self.offset_conv = layers.Conv2D(2 * self.kernel_size * self.kernel_size, (self.kernel_size, self.kernel_size), padding=padding, strides=strides)\n",
        "\n",
        "        # Convolution layer to generate modulation values (how much we scale the pixels)\n",
        "        self.modulation_conv = layers.Conv2D(self.kernel_size * self.kernel_size, (self.kernel_size, self.kernel_size), padding=padding, strides=strides)\n",
        "\n",
        "        # Final convolutional layer to generate the output features\n",
        "        self.main_conv = layers.Conv2D(filters, (self.kernel_size, self.kernel_size), padding=padding, strides=strides)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_shape = input_shape  # Save the input shape for future use\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Generate offsets for the convolution grid\n",
        "        offset = self.offset_conv(inputs)  # Shape: [batch_size, height, width, 2 * kernel_size^2]\n",
        "\n",
        "        # Generate modulation weights\n",
        "        modulation = tf.sigmoid(self.modulation_conv(inputs))  # Shape: [batch_size, height, width, kernel_size^2]\n",
        "\n",
        "        # Create a grid of indices based on the image dimensions\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        height, width = inputs.shape[1], inputs.shape[2]\n",
        "        grid = tf.meshgrid(\n",
        "            tf.range(height), tf.range(width), indexing='ij'\n",
        "        )\n",
        "        grid = tf.stack(grid, axis=-1)  # Shape becomes [height, width, 2]\n",
        "        grid = tf.cast(grid, tf.float32)  # Cast grid indices to float32\n",
        "\n",
        "        # Expand grid to match batch size: Shape becomes [batch_size, height, width, 2]\n",
        "        grid = tf.tile(tf.expand_dims(grid, 0), [batch_size, 1, 1, 1])\n",
        "\n",
        "        # Reshape and apply offsets to the grid, ensuring the dimensions match\n",
        "        offset = tf.reshape(offset, [batch_size, height, width, self.kernel_size, self.kernel_size, 2])  # Shape: [batch_size, height, width, kernel_size, kernel_size, 2]\n",
        "        grid_expanded = grid[..., None, None, :]  # Shape: [batch_size, height, width, 1, 1, 2]\n",
        "\n",
        "        offset_grid = grid_expanded + offset  # Shape becomes [batch_size, height, width, kernel_size, kernel_size, 2]\n",
        "\n",
        "        # Sample the input image based on the adjusted grid (deformable sampling)\n",
        "        sampled_inputs = self._deformable_sample(inputs, offset_grid)\n",
        "\n",
        "        # Ensure that modulation has the same spatial dimensions as sampled inputs\n",
        "        modulation = tf.image.resize(modulation, [sampled_inputs.shape[1], sampled_inputs.shape[2]])  # Resize modulation to match the height/width\n",
        "\n",
        "        # Scale the sampled inputs based on the modulation values\n",
        "        sampled_inputs = sampled_inputs * modulation\n",
        "\n",
        "        # Apply the final convolution layer\n",
        "        return self.main_conv(sampled_inputs)\n",
        "\n",
        "    def _deformable_sample(self, inputs, offset_grid):\n",
        "        \"\"\"Function to sample the inputs based on offset grid (not implemented here).\"\"\"\n",
        "        # This is a placeholder function for the deformable sampling process.\n",
        "        # In practice, you would need to implement or use a function to sample the input image\n",
        "        # based on the offset_grid. This is typically done using bilinear interpolation.\n",
        "        # For now, we'll assume that you have a working method for sampling.\n",
        "        return inputs  # Placeholder (in practice, you will sample using the offset_grid)\n",
        "\n",
        "\n",
        "# Step 2: Build the Generator Model\n",
        "def build_generator():\n",
        "    inputs = layers.Input(shape=(128, 128, 3))  # Input is an image of size 128x128 with 3 channels (RGB)\n",
        "\n",
        "    # Convolutional layer to downsample the input\n",
        "    x = layers.Conv2D(64, (3, 3), strides=2, padding='same', activation='relu')(inputs)\n",
        "    x = layers.BatchNormalization()(x)  # Apply batch normalization for stable training\n",
        "\n",
        "    # Deformable Convolution layer to further downsample with deformable convolution\n",
        "    x = DeformableConv2D(128, (3, 3), strides=2)(x)\n",
        "\n",
        "    # Upsampling via transposed convolution (deconvolution) to generate larger feature maps\n",
        "    x = layers.Conv2DTranspose(64, (3, 3), strides=2, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Final convolution layer to generate the output image\n",
        "    x = layers.Conv2D(3, (3, 3), padding='same', activation='tanh')(x)  # Output image with 3 channels (RGB)\n",
        "\n",
        "    return Model(inputs, x, name=\"Generator\")\n",
        "\n",
        "\n",
        "# Step 3: Build the Discriminator Model\n",
        "def build_discriminator():\n",
        "    inputs = layers.Input(shape=(128, 128, 3))  # Input is an image of size 128x128 with 3 channels (RGB)\n",
        "\n",
        "    # Convolutional layers to extract features from the image\n",
        "    x = layers.Conv2D(64, (3, 3), strides=2, padding='same', activation='relu')(inputs)\n",
        "    x = layers.BatchNormalization()(x)  # Batch normalization to stabilize training\n",
        "\n",
        "    x = layers.Conv2D(128, (3, 3), strides=2, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Flatten the output and apply a fully connected layer for classification\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)  # Sigmoid activation to classify between real (1) and fake (0)\n",
        "\n",
        "    return Model(inputs, x, name=\"Discriminator\")\n",
        "\n",
        "\n",
        "# Step 4: Compile the Models\n",
        "generator = build_generator()  # Create the generator\n",
        "discriminator = build_discriminator()  # Create the discriminator\n",
        "\n",
        "# Compile the discriminator with Adam optimizer and binary crossentropy loss\n",
        "discriminator.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy')\n",
        "discriminator.trainable = False  # Freeze discriminator during generator training\n",
        "\n",
        "# Create the GAN model by combining the generator and discriminator\n",
        "gan_input = layers.Input(shape=(128, 128, 3))  # Input is a random noise vector of size 128x128x3\n",
        "generated_image = generator(gan_input)  # Pass input noise through the generator\n",
        "gan_output = discriminator(generated_image)  # Classify the generated image using the discriminator\n",
        "gan = Model(gan_input, gan_output)\n",
        "\n",
        "# Compile the GAN model\n",
        "gan.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy')\n",
        "\n",
        "\n",
        "# Step 5: CIFAR-10 Dataset Preparation\n",
        "def preprocess_images(image):\n",
        "    # Resize the image to 128x128\n",
        "    image = tf.image.resize(image, [128, 128])\n",
        "    return (image - 127.5) / 127.5  # Normalize the pixel values to [-1, 1]\n",
        "\n",
        "# Load the CIFAR-10 dataset from TensorFlow's built-in datasets\n",
        "(x_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Preprocess and normalize the images\n",
        "x_train = preprocess_images(x_train)\n",
        "\n",
        "# Convert the images into a TensorFlow dataset and batch them\n",
        "dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "dataset = dataset.shuffle(1000).batch(32)  # Shuffle and batch the dataset\n",
        "\n",
        "\n",
        "# Step 6: Training Loop\n",
        "def train(generator, discriminator, gan, dataset, epochs=50):\n",
        "    for epoch in range(epochs):\n",
        "        for real_images in dataset:\n",
        "            batch_size = real_images.shape[0]\n",
        "            noise = tf.random.normal([batch_size, 128, 128, 3])  # Random noise to generate fake images\n",
        "            fake_images = generator(noise)  # Generate fake images from noise\n",
        "\n",
        "            # Labels for real and fake images\n",
        "            real_labels = tf.ones((batch_size, 1))  # Label for real images\n",
        "            fake_labels = tf.zeros((batch_size, 1))  # Label for fake images\n",
        "\n",
        "            # Train the discriminator\n",
        "            with tf.GradientTape() as tape:\n",
        "                real_output = discriminator(real_images)  # Get discriminator output for real images\n",
        "                fake_output = discriminator(fake_images)  # Get discriminator output for fake images\n",
        "                d_loss_real = tf.keras.losses.binary_crossentropy(real_labels, real_output)\n",
        "                d_loss_fake = tf.keras.losses.binary_crossentropy(fake_labels, fake_output)\n",
        "                d_loss = d_loss_real + d_loss_fake  # Total discriminator loss\n",
        "            grads = tape.gradient(d_loss, discriminator.trainable_variables)  # Compute gradients\n",
        "            discriminator.optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))  # Apply gradients\n",
        "\n",
        "            # Train the generator (via the GAN model)\n",
        "            with tf.GradientTape() as tape:\n",
        "                fake_images = generator(noise)  # Generate fake images\n",
        "                fake_output = discriminator(fake_images)  # Get discriminator output for fake images\n",
        "                g_loss = tf.keras.losses.binary_crossentropy(real_labels, fake_output)  # Generator loss\n",
        "            grads = tape.gradient(g_loss, generator.trainable_variables)  # Compute gradients\n",
        "            gan.optimizer.apply_gradients(zip(grads, generator.trainable_variables))  # Apply gradients\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs} | D Loss: {d_loss.numpy()} | G Loss: {g_loss.numpy()}')\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            generate_images(generator)\n",
        "\n",
        "\n",
        "# Generate images function for visualization\n",
        "def generate_images(generator, num_images=10):\n",
        "    noise = tf.random.normal((num_images, 128, 128, 3))  # Random noise\n",
        "    generated_images = generator(noise)  # Generate images from noise\n",
        "\n",
        "    # Rescale the images to [0, 255] range for visualization\n",
        "    generated_images = (generated_images + 1) * 127.5  # Rescale from [-1, 1] to [0, 255]\n",
        "\n",
        "    # Display the generated images\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(1, num_images, i+1)\n",
        "        plt.imshow(generated_images[i].numpy().astype(\"uint8\"))\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Step 7: Start training the GAN\n",
        "train(generator, discriminator, gan, dataset, epochs=50)\n",
        "\n",
        "# Generate and visualize some fake images after training\n",
        "generate_images(generator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "H7QDD-5HP4hu",
        "outputId": "906dc9c6-7bac-445a-b642-8a196b800282"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling DeformableConv2D.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'deformable_conv2d_10' (of type DeformableConv2D). Either the `DeformableConv2D.call()` method is incorrect, or you need to implement the `DeformableConv2D.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 64 and 9 for '{{node mul}} = Mul[T=DT_FLOAT](Placeholder, resize/ResizeBilinear)' with input shapes: [?,64,64,64], [?,64,64,9].\u001b[0m\n\nArguments received by DeformableConv2D.call():\n  • args=('<KerasTensor shape=(None, 64, 64, 64), dtype=float32, sparse=False, name=keras_tensor_32>',)\n  • kwargs=<class 'inspect._empty'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a0b616f4b740>\u001b[0m in \u001b[0;36m<cell line: 110>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m# Step 4: Compile the Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-a0b616f4b740>\u001b[0m in \u001b[0;36mbuild_generator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Deformable Convolution layer to further downsample with deformable convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeformableConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# Upsampling via transposed convolution (deconvolution) to generate larger feature maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-a0b616f4b740>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Scale the sampled inputs based on the modulation values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0msampled_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled_inputs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmodulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Apply the final convolution layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling DeformableConv2D.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'deformable_conv2d_10' (of type DeformableConv2D). Either the `DeformableConv2D.call()` method is incorrect, or you need to implement the `DeformableConv2D.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 64 and 9 for '{{node mul}} = Mul[T=DT_FLOAT](Placeholder, resize/ResizeBilinear)' with input shapes: [?,64,64,64], [?,64,64,9].\u001b[0m\n\nArguments received by DeformableConv2D.call():\n  • args=('<KerasTensor shape=(None, 64, 64, 64), dtype=float32, sparse=False, name=keras_tensor_32>',)\n  • kwargs=<class 'inspect._empty'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GinhcgHNZaPB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}