{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "j211oDpvzmuC"
      },
      "outputs": [],
      "source": [
        "# Import TensorFlow for model building and training\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HierarchicalVAE(tf.keras.Model):\n",
        "    # Initialize model with input and latent dimensions\n",
        "    def __init__(self, input_dim, latent_dim_1, latent_dim_2):\n",
        "        super(HierarchicalVAE, self).__init__()\n",
        "\n",
        "        # Encoder for the first level, converting input to first latent representation\n",
        "        self.encoder_level1 = tf.keras.Sequential([\n",
        "            layers.InputLayer(input_shape=input_dim),\n",
        "            layers.Dense(256, activation='relu'),  # First hidden layer\n",
        "            layers.Dense(128, activation='relu'),  # Second hidden layer\n",
        "            layers.Dense(latent_dim_1 * 2)         # Output for mean and log variance\n",
        "        ])\n",
        "\n",
        "        # Encoder for the second level, converting first latent to deeper latent representation\n",
        "        self.encoder_level2 = tf.keras.Sequential([\n",
        "            layers.InputLayer(input_shape=(latent_dim_1,)),\n",
        "            layers.Dense(64, activation='relu'),    # Hidden layer for latent processing\n",
        "            layers.Dense(latent_dim_2 * 2)          # Output for mean and log variance\n",
        "        ])\n",
        "\n",
        "        # Decoder for the second level, reconstructing latent_dim_1 from latent_dim_2\n",
        "        self.decoder_level2 = tf.keras.Sequential([\n",
        "            layers.InputLayer(input_shape=(latent_dim_2,)),\n",
        "            layers.Dense(64, activation='relu'),    # Hidden layer for decoding\n",
        "            layers.Dense(latent_dim_1)              # Reconstructed latent_dim_1 output\n",
        "        ])\n",
        "\n",
        "        # Decoder for the first level, reconstructing the input from latent_dim_1\n",
        "        self.decoder_level1 = tf.keras.Sequential([\n",
        "            layers.InputLayer(input_shape=(latent_dim_1,)),\n",
        "            layers.Dense(128, activation='relu'),   # First hidden layer\n",
        "            layers.Dense(256, activation='relu'),   # Second hidden layer\n",
        "            layers.Dense(input_dim, activation='sigmoid')  # Final output layer\n",
        "        ])\n"
      ],
      "metadata": {
        "id": "lMl8KxPxzqhV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def sample(self, mean, log_var):\n",
        "        \"\"\"\n",
        "        Reparameterization trick to sample from N(mean, var) using N(0,1).\n",
        "        \"\"\"\n",
        "        epsilon = tf.random.normal(shape=tf.shape(mean))  # Sample from standard normal\n",
        "        return mean + tf.exp(0.5 * log_var) * epsilon     # Scale and shift to match mean and log_var\n"
      ],
      "metadata": {
        "id": "lJRKoinKzqec"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def encode(self, x):\n",
        "        \"\"\"\n",
        "        Encodes input x into hierarchical latent representations at two levels.\n",
        "        \"\"\"\n",
        "        # Level 1 encoding\n",
        "        h1 = self.encoder_level1(x)\n",
        "        mean1, log_var1 = tf.split(h1, num_or_size_splits=2, axis=1)  # Split into mean and log variance\n",
        "        z1 = self.sample(mean1, log_var1)  # Sample first latent z1\n",
        "\n",
        "        # Level 2 encoding\n",
        "        h2 = self.encoder_level2(z1)\n",
        "        mean2, log_var2 = tf.split(h2, num_or_size_splits=2, axis=1)  # Split for mean and log variance\n",
        "        z2 = self.sample(mean2, log_var2)  # Sample second latent z2\n",
        "\n",
        "        return mean1, log_var1, z1, mean2, log_var2, z2\n"
      ],
      "metadata": {
        "id": "ocSUMnpxzqbf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def decode(self, z1, z2):\n",
        "        \"\"\"\n",
        "        Decodes hierarchical latents z2 and z1 back to the input space.\n",
        "        \"\"\"\n",
        "        z1_reconstructed = self.decoder_level2(z2)  # Reconstruct z1 from z2\n",
        "        x_reconstructed = self.decoder_level1(z1_reconstructed)  # Reconstruct input from z1\n",
        "        return x_reconstructed\n"
      ],
      "metadata": {
        "id": "Z88Rj1TMzqY0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def call(self, x):\n",
        "        mean1, log_var1, z1, mean2, log_var2, z2 = self.encode(x)\n",
        "        x_reconstructed = self.decode(z1, z2)\n",
        "        return x_reconstructed, mean1, log_var1, mean2, log_var2\n"
      ],
      "metadata": {
        "id": "JMZUgv9NzqWM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(model, x):\n",
        "    # Forward pass to get reconstructed x and latent parameters\n",
        "    x_reconstructed, mean1, log_var1, mean2, log_var2 = model(x)\n",
        "\n",
        "    # Binary cross-entropy for reconstruction loss\n",
        "    reconstruction_loss = tf.reduce_mean(\n",
        "        tf.reduce_sum(tf.keras.losses.binary_crossentropy(x, x_reconstructed), axis=1)\n",
        "    )\n",
        "\n",
        "    # KL divergence for Level 1 latent space\n",
        "    kl_div1 = -0.5 * tf.reduce_sum(1 + log_var1 - tf.square(mean1) - tf.exp(log_var1), axis=1)\n",
        "\n",
        "    # KL divergence for Level 2 latent space\n",
        "    kl_div2 = -0.5 * tf.reduce_sum(1 + log_var2 - tf.square(mean2) - tf.exp(log_var2), axis=1)\n",
        "\n",
        "    kl_loss = tf.reduce_mean(kl_div1 + kl_div2)  # Total KL loss for both levels\n",
        "    return reconstruction_loss + kl_loss         # Total VAE loss\n"
      ],
      "metadata": {
        "id": "BJvXGtWzzxvF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, x):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = compute_loss(model, x)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "ZUYBoa9Izxse"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataset, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        for step, x_batch in enumerate(dataset):\n",
        "            loss = train_step(model, x_batch)\n",
        "        print(f'Epoch {epoch + 1}, Loss: {loss.numpy()}')\n"
      ],
      "metadata": {
        "id": "LZsGrMqYzxpb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jdtBCd090JBA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}